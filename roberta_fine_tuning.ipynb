{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"name":"roberta_fine_tuning.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"20aDfXGcAGq4","colab_type":"text"},"source":["**Add features on top of BERT fine tunning, see reference**\n","\n","https://datascience.stackexchange.com/questions/54888/how-can-i-add-custom-numerical-features-for-training-to-bert-fine-tuning\n","\n","https://www.pyimagesearch.com/2019/02/04/keras-multiple-inputs-and-mixed-data/\n","\n","Binary_crossentropy: https://gombru.github.io/2018/05/23/cross_entropy_loss/"]},{"cell_type":"code","metadata":{"id":"F-LCG6ynAmjv","colab_type":"code","outputId":"e19f7b7a-6ee0-4acc-ed64-a285046d3bf0","executionInfo":{"status":"ok","timestamp":1590717833755,"user_tz":240,"elapsed":32803,"user":{"displayName":"Dechang Chen","photoUrl":"","userId":"17152161433207141669"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VOM0pUojDLQM","colab_type":"code","outputId":"bb07d7e8-1295-4aed-e831-8b8466a102f5","executionInfo":{"status":"ok","timestamp":1590717842660,"user_tz":240,"elapsed":6262,"user":{"displayName":"Dechang Chen","photoUrl":"","userId":"17152161433207141669"}},"colab":{"base_uri":"https://localhost:8080/","height":561}},"source":["!pip install transformers"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/b5/ac41e3e95205ebf53439e4dd087c58e9fd371fd8e3724f2b9b4cdb8282e5/transformers-2.10.0-py3-none-any.whl (660kB)\n","\u001b[K     |████████████████████████████████| 665kB 3.4MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 15.6MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 26.9MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Collecting tokenizers==0.7.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 29.9MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=cf464331f058fcd0209973841517682d17334a2350a220adb37bca700b745da0\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.10.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ettAIDi-CADj","colab_type":"code","colab":{}},"source":["ROOT_PATH = '/content/drive/My Drive/Qishi/NLP/project'\n","# ROOT_PATH = '/kaggle'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"id":"5AlHAPzoAGq6","colab_type":"code","outputId":"1bffbe5a-3198-4f02-c364-92702334101d","executionInfo":{"status":"ok","timestamp":1590717843812,"user_tz":240,"elapsed":1548,"user":{"displayName":"Dechang Chen","photoUrl":"","userId":"17152161433207141669"}},"colab":{"base_uri":"https://localhost:8080/","height":255}},"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk(ROOT_PATH+'/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Qishi/NLP/project/input/tweet-sentiment-extraction/train.csv\n","/content/drive/My Drive/Qishi/NLP/project/input/tweet-sentiment-extraction/sample_submission.csv\n","/content/drive/My Drive/Qishi/NLP/project/input/tweet-sentiment-extraction/test.csv\n","/content/drive/My Drive/Qishi/NLP/project/input/tfrobert/glove_6B_100d_top100k.csv\n","/content/drive/My Drive/Qishi/NLP/project/input/tfrobert/roberta-base-vocab.json\n","/content/drive/My Drive/Qishi/NLP/project/input/tfrobert/roberta-base-merges.txt\n","/content/drive/My Drive/Qishi/NLP/project/input/tfrobert/pretrained-roberta-base.h5\n","/content/drive/My Drive/Qishi/NLP/project/input/tfrobert/roberta-base-tf_model.h5\n","/content/drive/My Drive/Qishi/NLP/project/input/tfrobert/config-roberta-base.json\n","/content/drive/My Drive/Qishi/NLP/project/input/tfrobert/roberta-base-config.json\n","/content/drive/My Drive/Qishi/NLP/project/input/roberta-base/pytorch_model.bin\n","/content/drive/My Drive/Qishi/NLP/project/input/roberta-base/config.json\n","/content/drive/My Drive/Qishi/NLP/project/input/roberta-base/merges.txt\n","/content/drive/My Drive/Qishi/NLP/project/input/roberta-base/vocab.json\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"id":"ilDKj7ZfAGq_","colab_type":"code","outputId":"15d376f7-d58c-4795-bcad-eca1668778b0","executionInfo":{"status":"ok","timestamp":1590717856701,"user_tz":240,"elapsed":5001,"user":{"displayName":"Dechang Chen","photoUrl":"","userId":"17152161433207141669"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import pandas as pd, numpy as np\n","import tensorflow as tf\n","import tensorflow.keras.backend as K\n","from sklearn.model_selection import StratifiedKFold, ShuffleSplit\n","from transformers import *\n","import tokenizers\n","import gc\n","from tensorflow.python.framework import ops\n","print('TF version',tf.__version__)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["TF version 2.2.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qeImukR-LDvg","colab_type":"code","colab":{}},"source":["a = np.array([[0, 1], [0, 1]])\n","b = np.array([[0, 1], [0, 1]])\n","ab = np.concatenate([a, b], axis=-1)\n","c = np.array([[0.2, 0.8], [0.2, 0.8]])\n","d= np.array([[0, 1.0], [0, 1.0]])\n","cd = np.concatenate([c, d], axis=-1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gYSWxQoqL_-q","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":139},"outputId":"84d480a2-27ee-4e0a-b7d5-916a78a84a12","executionInfo":{"status":"ok","timestamp":1590718590275,"user_tz":240,"elapsed":335,"user":{"displayName":"Dechang Chen","photoUrl":"","userId":"17152161433207141669"}}},"source":["ab = Concatenate(axis=-1)([a, b])\n","cd = Concatenate(axis=-1)([c, d])"],"execution_count":16,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Layer concatenate_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n","\n","If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n","\n","To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"U_wxfSvENFqO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"1bff3f08-0620-4c40-b078-e7995c881572","executionInfo":{"status":"ok","timestamp":1590718632145,"user_tz":240,"elapsed":285,"user":{"displayName":"Dechang Chen","photoUrl":"","userId":"17152161433207141669"}}},"source":["tf.keras.losses.CategoricalCrossentropy()(K.cast(ab[:, :2], dtype='float32'), cd[:,:2])"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(), dtype=float32, numpy=0.22314353>"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"trusted":true,"id":"gL1GtYaXAGrE","colab_type":"code","colab":{}},"source":["from tensorflow.keras.layers import Input, Dropout, Conv1D, LeakyReLU, Dense, Flatten, Activation, Reshape, Concatenate, Bidirectional, LSTM\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","\n","learning_rate_fn = tf.keras.optimizers.schedules.PolynomialDecay(\n","    initial_learning_rate=2e-5,\n","    decay_steps=10000,\n","    end_learning_rate=1e-6,\n","    power=1)\n","\n","def jaccard(str1, str2): \n","    a = set(str1.lower().split()) \n","    b = set(str2.lower().split())\n","    if (len(a)==0) & (len(b)==0): return 0.5\n","    c = a.intersection(b)\n","    return float(len(c)) / (len(a) + len(b) - len(c))\n","\n","\n","class FineTuneBertForQA:\n","    def __init__(self, tokenizer, config_file, bert_model_file, max_length, n_splits, loss_fn='categorical_crossentropy', separated_output=True, seed=777):\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","        self.sentiment_id = {s:self.tokenizer.encode(s).ids[0] for s in ['positive', 'negative', 'neutral']}\n","        self.config_file = config_file\n","        self.bert_model_file = bert_model_file\n","        self.n_splits = n_splits\n","        self.loss_fn = loss_fn\n","        self.separated_output = separated_output\n","        self.seed = seed\n","\n","    def bert_data_transform(self, data, train=True):\n","        '''\n","        Transform data into arrays that BERT understands \n","        '''\n","        ct = data.shape[0]\n","        input_ids = np.ones((ct,self.max_length),dtype='int32')\n","        attention_mask = np.zeros((ct,self.max_length),dtype='int32')\n","        #token_type_ids = np.zeros((ct,self.max_length),dtype='int32')\n","        if train:\n","            if self.separated_output:\n","                start_tokens = np.zeros((ct,self.max_length),dtype='int32')\n","                end_tokens = np.zeros((ct,self.max_length),dtype='int32')\n","            else:\n","                span_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n","\n","        for k in range(ct):\n","            # FIND OVERLAP\n","            text1 = \" \"+\" \".join(data.loc[k,'text'].split())\n","            enc = self.tokenizer.encode(text1)\n","            s_tok = self.sentiment_id[data.loc[k,'sentiment']]\n","            input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n","            attention_mask[k,:len(enc.ids)+5] = 1\n","            \n","            if train:\n","                text2 = \" \".join(data.loc[k,'selected_text'].split())\n","                idx = text1.find(text2)\n","                chars = np.zeros((len(text1)))\n","                chars[idx:idx+len(text2)]=1\n","                if text1[idx-1]==' ': chars[idx-1] = 1 \n","                # ID_OFFSETS\n","                offsets = []; idx=0\n","                for t in enc.ids:\n","                    w = self.tokenizer.decode([t])\n","                    offsets.append((idx,idx+len(w)))\n","                    idx += len(w)\n","\n","                # START END TOKENS\n","                toks = []\n","                for i,(a,b) in enumerate(offsets):\n","                    sm = np.sum(chars[a:b])\n","                    if sm>0: toks.append(i) \n","\n","                if len(toks)>0:\n","                    if self.separated_output:\n","                        start_tokens[k,toks[0]+1] = 1\n","                        end_tokens[k,toks[-1]+1] = 1\n","                    else:\n","                        for i in range(toks[0], toks[-1]+1):\n","                            span_tokens[k,i+1] = 1\n","\n","        if train:\n","            if self.separated_output:\n","                return (input_ids, attention_mask, start_tokens, end_tokens)\n","            else:\n","                return (input_ids, attention_mask, span_tokens)\n","        else:\n","            return (input_ids, attention_mask)\n","    \n","    def build_model(self):\n","        '''\n","        Add layer on top of BERT\n","        '''        \n","        ids = Input((self.max_length,), dtype=tf.int32)\n","        att = Input((self.max_length,), dtype=tf.int32)\n","        #tok = Input((self.length,), dtype=tf.int32)\n","        \n","        config = RobertaConfig.from_pretrained(self.config_file)\n","        config.output_hidden_states=True\n","        bert_model = TFRobertaModel.from_pretrained(self.bert_model_file,config=config)\n","        \n","        _ , _ , x = bert_model(ids,attention_mask=att)\n","        \n","        def output_layer(bert_output, name='start', activation='softmax'):\n","            x_bert = K.max(K.stack(bert_output), axis=0)\n","            x_bert = Dropout(0.5)(x_bert)\n","            #x_bert = Dropout(0.1)(bert_output)\n","            #x_bert = Conv1D(128, 2,padding='same')(x_bert)\n","            #x_bert = LeakyReLU()(x_bert)\n","            #x_bert = Conv1D(64, 2, padding='same')(x_bert)\n","            #x_bert = Conv1D(1, 2, padding='same')(x_bert)\n","            x_bert = Dense(1)(x_bert)                       \n","            x_output = Flatten()(x_bert)\n","            x_output = Activation(activation, name=name)(x_output)\n","             \n","            return x_output\n","        \n","        if self.separated_output:\n","            #x1_output = output_layer(x[0], name='start')\n","            #x2_output = output_layer(x[0], name='end')\n","            x1_output = output_layer([x[-1], x[-2], x[-3]], name='start')\n","            x2_output = output_layer([x[-1], x[-2], x[-3]], name='end')\n","    \n","            model = Model(inputs=[ids, att], outputs=[x1_output,x2_output])\n","            # optimizer = Adam(learning_rate=3e-5)\n","            # model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n","        else:\n","            #x_output = output_layer(x[0], name='span', activation='sigmoid')\n","            x_output = output_layer([x[-1], x[-2], x[-3]], name='span', activation='sigmoid')   \n","            model = Model(inputs=[ids, att], outputs=x_output)\n","\n","        #optimizer = Adam(learning_rate=1.5e-5)\n","        optimizer = Adam(learning_rate=2.4e-5)\n","        model.compile(loss=self.loss_fn, optimizer=optimizer)\n","        #model.compile(loss=jaccard_distance, optimizer=optimizer)\n","        \n","        # print(model.summary)\n","        # K.clear_session()\n","            \n","        return model\n","    \n","    def predict_decode(self, text_data, preds, vec_idx):\n","        all_selected_text = []\n","        ii = 0\n","        if self.separated_output:\n","            start = preds[0]\n","            end = preds[1]\n","        else:\n","            span = preds[0]\n","\n","        for k in vec_idx:\n","            if self.separated_output:\n","                a = np.argmax(start[k,])\n","                b = np.argmax(end[k,])\n","            else:\n","                min_prob = np.min(span[k, ])\n","                max_prob = np.max(span[k, ])\n","                span_idx = np.where(span[k, ]>0.5*(max_prob-min_prob)+min_prob)[0]\n","                if len(span_idx)==0:\n","                    a=0\n","                    b=-1\n","                else:\n","                    a = span_idx[0]\n","                    b = span_idx[-1]\n","            if a>b: \n","                ii = ii + 1\n","                st = text_data.loc[k,'text']\n","            else:\n","                text1 = \" \"+\" \".join(text_data.loc[k,'text'].split())\n","                enc = self.tokenizer.encode(text1)\n","                st = self.tokenizer.decode(enc.ids[a-1:b])\n","            all_selected_text.append(st)\n","        print(ii)\n","        return all_selected_text\n","\n","    def train_model(self, train, epochs=3, batch_size=32, version='v0', model_path=None, verbose=1):\n","        # USE verbose=1 FOR INTERACTIVE\n","        train = train.reset_index(drop=True)\n","        if self.separated_output:\n","            input_ids, attention_mask, start_tokens, end_tokens = self.bert_data_transform(train, train=True)\n","            oof_start = np.zeros((input_ids.shape[0],self.max_length))\n","            oof_end = np.zeros((input_ids.shape[0],self.max_length))\n","        else:\n","            input_ids, attention_mask, span_tokens = self.bert_data_transform(train, train=True)\n","            oof_span = np.zeros((input_ids.shape[0],self.max_length))\n","\n","        jacs = [] \n","                \n","        if self.n_splits == 1:\n","            rs = ShuffleSplit(n_splits=1, test_size=0.25, random_state=self.seed)\n","        else:\n","            rs = StratifiedKFold(n_splits=self.n_splits,shuffle=True,random_state=self.seed)\n","            \n","        for fold,(idxT,idxV) in enumerate(rs.split(input_ids,train.sentiment.values)):\n","            print('#'*25)\n","            print('### FOLD %i'%(fold+1))\n","            print('#'*25)\n","           \n","            K.clear_session()\n","            model = self.build_model()\n","            print(model.summary)\n","\n","            if model_path is None:\n","                print('Training model...')\n","                #et = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto')\n","                sv = ModelCheckpoint('%s-roberta-%i.h5'%(version,fold), monitor='val_loss', verbose=1, save_best_only=True,\n","                                     save_weights_only=True, mode='auto', save_freq='epoch')\n","                if self.separated_output:\n","                    targetT = [start_tokens[idxT,], end_tokens[idxT,]]\n","                    targetV = [start_tokens[idxV,], end_tokens[idxV,]]\n","                else:\n","                    targetT = span_tokens[idxT,]\n","                    targetV = span_tokens[idxV,]\n","\n","                model.fit([input_ids[idxT,], attention_mask[idxT,]], targetT, \n","                          epochs=epochs, batch_size=batch_size, verbose=verbose, callbacks=[sv],\n","                          validation_data=([input_ids[idxV,],attention_mask[idxV,]], targetV))\n","                #print('Loading model...')\n","                #model.load_weights('%s-roberta-%i.h5'%(version,fold))\n","            else:\n","                print('Loading model...')\n","                model.load_weights('%s/%s-roberta-%i.h5'%(model_path,version,fold))\n","            \n","            for label, idx in {'INF':idxT, 'OOF':idxV}.items():\n","                print('Predicting %s...'%label)\n","                if self.separated_output:\n","                    oof_start[idx,],oof_end[idx,] = model.predict([input_ids[idx,],attention_mask[idx,]],verbose=verbose)\n","                else:\n","                    oof_span[idx,] = model.predict([input_ids[idx,],attention_mask[idx,]],verbose=verbose)\n","                # DISPLAY FOLD JACCARD\n","                if self.separated_output:\n","                    all_selected_text = self.predict_decode(train, [oof_start, oof_end], idx)\n","                else:\n","                    all_selected_text = self.predict_decode(train, [oof_span], idx)\n","                all_jac = []\n","                for i in range(len(idx)):\n","                    all_jac.append(jaccard(all_selected_text[i],train.loc[idx[i],'selected_text']))\n","                print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all_jac))\n","                print()\n","                \n","                if label == 'OOF':\n","                    jacs.append(np.mean(all_jac))\n","                                 \n","        print('>>>> OVERALL %i Fold CV Jaccard ='%self.n_splits,np.mean(jacs))\n","        \n","        \n","    def predict_test(self, test, version='v0', model_path=None, verbose=1):\n","        test = test.reset_index(drop=True)\n","        input_ids, attention_mask = self.bert_data_transform(test, train=False)\n","        \n","        if self.separated_output:\n","            preds_start = np.zeros((input_ids.shape[0],self.max_length))\n","            preds_end = np.zeros((input_ids.shape[0],self.max_length))\n","        else:\n","            preds_span = np.zeros((input_ids.shape[0],self.max_length))\n","\n","        for fold in range(self.n_splits):\n","            K.clear_session()\n","            model = self.build_model()\n","            if model_path is None:\n","                model.load_weights('%s-roberta-%i.h5'%(version,fold))\n","            else:\n","                model.load_weights('%s/%s-roberta-%i.h5'%(model_path,version,fold))\n","            print('Predicting Test...')\n","            preds = model.predict([input_ids,attention_mask],verbose=verbose)\n","            if self.separated_output:\n","                preds_start += preds[0]/self.n_splits\n","                preds_end += preds[1]/self.n_splits\n","            else:\n","                preds_span += preds/self.n_splits\n","        \n","        if self.separated_output:                \n","            all_selected_text = self.predict_decode(test, [preds_start, preds_end], test.index)\n","        else:\n","            all_selected_text = self.predict_decode(test, [preds_span], test.index)\n","\n","        return all_selected_text"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zCNS-mliAGrI","colab_type":"text"},"source":["# Load data"]},{"cell_type":"code","metadata":{"trusted":true,"id":"1Kx79ayDAGrI","colab_type":"code","colab":{}},"source":["train = pd.read_csv(ROOT_PATH + '/input/tweet-sentiment-extraction/train.csv').fillna('')\n","test = pd.read_csv(ROOT_PATH + '/input/tweet-sentiment-extraction/test.csv').fillna('')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"qBMTQYOqAGrN","colab_type":"code","outputId":"4ce19881-669a-405f-a36d-8ca8c7ad2a8d","executionInfo":{"status":"ok","timestamp":1590423744499,"user_tz":240,"elapsed":2459,"user":{"displayName":"Dechang Chen","photoUrl":"","userId":"17152161433207141669"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["train.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(27481, 4)"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"trusted":true,"id":"k6tDiMlLAGrQ","colab_type":"code","outputId":"34aa89fd-0685-47bf-92c0-bb0fdf1ec03c","executionInfo":{"status":"ok","timestamp":1590423748882,"user_tz":240,"elapsed":4258,"user":{"displayName":"Dechang Chen","photoUrl":"","userId":"17152161433207141669"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["train.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>textID</th>\n","      <th>text</th>\n","      <th>selected_text</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>cb774db0d1</td>\n","      <td>I`d have responded, if I were going</td>\n","      <td>I`d have responded, if I were going</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>549e992a42</td>\n","      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n","      <td>Sooo SAD</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>088c60f138</td>\n","      <td>my boss is bullying me...</td>\n","      <td>bullying me</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>9642c003ef</td>\n","      <td>what interview! leave me alone</td>\n","      <td>leave me alone</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>358bd9e861</td>\n","      <td>Sons of ****, why couldn`t they put them on t...</td>\n","      <td>Sons of ****,</td>\n","      <td>negative</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       textID  ... sentiment\n","0  cb774db0d1  ...   neutral\n","1  549e992a42  ...  negative\n","2  088c60f138  ...  negative\n","3  9642c003ef  ...  negative\n","4  358bd9e861  ...  negative\n","\n","[5 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"S39fn5AjfwXr","colab_type":"code","outputId":"bb2717e6-5b5c-4dc2-bdcf-b16c8ab2014d","executionInfo":{"status":"ok","timestamp":1590545906493,"user_tz":240,"elapsed":1012,"user":{"displayName":"Dechang Chen","photoUrl":"","userId":"17152161433207141669"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["dummy_jacs = []\n","for i in range(train.shape[0]):\n","  jac = jaccard(train.loc[i, 'text'], train.loc[i, 'selected_text'])\n","  dummy_jacs.append(jac)\n","np.mean(dummy_jacs)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.589073146730252"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"trusted":true,"id":"muS-eLAQAGrU","colab_type":"code","outputId":"966524d2-95fc-41ce-8338-5849e7f369e2","executionInfo":{"status":"ok","timestamp":1590423750553,"user_tz":240,"elapsed":2266,"user":{"displayName":"Dechang Chen","photoUrl":"","userId":"17152161433207141669"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["test.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>textID</th>\n","      <th>text</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>f87dea47db</td>\n","      <td>Last session of the day  http://twitpic.com/67ezh</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>96d74cb729</td>\n","      <td>Shanghai is also really exciting (precisely -...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>eee518ae67</td>\n","      <td>Recession hit Veronique Branquinho, she has to...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>01082688c6</td>\n","      <td>happy bday!</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>33987a8ee5</td>\n","      <td>http://twitpic.com/4w75p - I like it!!</td>\n","      <td>positive</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       textID                                               text sentiment\n","0  f87dea47db  Last session of the day  http://twitpic.com/67ezh   neutral\n","1  96d74cb729   Shanghai is also really exciting (precisely -...  positive\n","2  eee518ae67  Recession hit Veronique Branquinho, she has to...  negative\n","3  01082688c6                                        happy bday!  positive\n","4  33987a8ee5             http://twitpic.com/4w75p - I like it!!  positive"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"T-6SzG16AGrZ","colab_type":"text"},"source":["# Setup"]},{"cell_type":"code","metadata":{"trusted":true,"id":"60e-fThAAGra","colab_type":"code","colab":{}},"source":["PATH = ROOT_PATH + '/input/tfrobert/'\n","tokenizer = tokenizers.ByteLevelBPETokenizer(\n","    vocab_file=PATH+'/roberta-base-vocab.json', \n","    merges_file=PATH+'/roberta-base-merges.txt', \n","    lowercase=True,\n","    add_prefix_space=True\n",")\n","\n","# config = RobertaConfig.from_pretrained(PATH+'roberta-base-config.json')\n","# bert_model = TFRobertaModel.from_pretrained(PATH+'roberta-base-tf_model.h5',config=config)\n","\n","# config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n","# bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n","\n","SAMPLE_RUN = False # Set True if you just want to debug implementation. Otherwise False.\n","MODEL_PATH = None # Set None if you want to train a new model, otherwise specify the PATH of trained model\n","N_SPLITS = 10 # 1 means train validation split at portion 0.75:0.25\n","MAX_LEN = 128\n","SEP_OUTPUT = True # True: if separate predictions for start and end, Flase: predict everything at all"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tixkicfZAGrd","colab_type":"text"},"source":["# Train model"]},{"cell_type":"code","metadata":{"id":"FwmxCzyf7Pwc","colab_type":"code","colab":{}},"source":["def jaccard_distance(y_true, y_pred, smooth=1):\n","    y_true = tf.keras.backend.cast(y_true, dtype='float32')\n","    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n","    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n","    #jac = (intersection + smooth) / (sum_ - intersection + smooth)\n","    return -K.log(intersection + smooth) + K.log(sum_ - intersection + smooth)\n","\n","def smoothed_crossentropy(y_true, y_pred):\n","    # adjust the targets for sequence bucketing\n","    ll = tf.shape(y_pred)[1]\n","    y_true = y_true[:, :ll]\n","    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred,\n","        from_logits=False, label_smoothing=0.1)\n","    loss = tf.reduce_mean(loss)\n","    return loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"Trxw6yUBAGre","colab_type":"code","outputId":"1f1ce5fe-bea0-4243-b942-9a2b44c9e9de","executionInfo":{"status":"ok","timestamp":1590564497688,"user_tz":240,"elapsed":18547875,"user":{"displayName":"Dechang Chen","photoUrl":"","userId":"17152161433207141669"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["SEED = 43\n","tf.random.set_seed(SEED)\n","np.random.seed(SEED)\n","\n","if SAMPLE_RUN:\n","    sample_train = train.sample(2000, random_state=SEED)\n","    sample_test = test.sample(3, random_state=SEED)\n","\n","    sample_model = FineTuneBertForQA(tokenizer, config_file=PATH+'roberta-base-config.json', \n","                                       bert_model_file=PATH+'roberta-base-tf_model.h5', \n","                                       max_length=MAX_LEN, n_splits=N_SPLITS, loss_fn=\"categorical_crossentropy\", separated_output=SEP_OUTPUT, seed=SEED)\n","    sample_model.train_model(sample_train, epochs=2, batch_size=32, version='v1', model_path=MODEL_PATH, verbose=1)\n","    \n","    sample_selected_text = sample_model.predict_test(sample_test, version='v1', model_path=MODEL_PATH, verbose=1)\n","    sample_test['selected_text'] = sample_selected_text\n","    pd.set_option('max_colwidth', 60)\n","    print(sample_test)\n","else:\n","    full_model = FineTuneBertForQA(tokenizer, config_file=PATH+'config-roberta-base.json', \n","                                     bert_model_file=PATH+'pretrained-roberta-base.h5', \n","                                     max_length=MAX_LEN, n_splits=N_SPLITS, loss_fn=\"categorical_crossentropy\", separated_output=SEP_OUTPUT, seed=SEED)\n","    full_model.train_model(train, epochs=4, batch_size=16, version='v4', model_path=MODEL_PATH, verbose=1)\n","\n","    all_selected_text = full_model.predict_test(test, version='v4', model_path=MODEL_PATH, verbose=1)\n","    test['selected_text'] = all_selected_text\n","    test[['textID','selected_text']].to_csv('submission.csv',index=False)\n","    pd.set_option('max_colwidth', 60)\n","    print(test.sample(25))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["#########################\n","### FOLD 1\n","#########################\n","<bound method Network.summary of <tensorflow.python.keras.engine.training.Model object at 0x7f5a90273da0>>\n","Training model...\n","Epoch 1/4\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","1546/1546 [==============================] - ETA: 0s - loss: 2.3122 - start_loss: 1.1320 - end_loss: 1.1802\n","Epoch 00001: val_loss improved from inf to 1.69637, saving model to v4-roberta-0.h5\n","1546/1546 [==============================] - 417s 270ms/step - loss: 2.3122 - start_loss: 1.1320 - end_loss: 1.1802 - val_loss: 1.6964 - val_start_loss: 0.8718 - val_end_loss: 0.8246\n","Epoch 2/4\n","1546/1546 [==============================] - ETA: 0s - loss: 1.7587 - start_loss: 0.8833 - end_loss: 0.8754\n","Epoch 00002: val_loss improved from 1.69637 to 1.68213, saving model to v4-roberta-0.h5\n","1546/1546 [==============================] - 414s 268ms/step - loss: 1.7587 - start_loss: 0.8833 - end_loss: 0.8754 - val_loss: 1.6821 - val_start_loss: 0.8560 - val_end_loss: 0.8261\n","Epoch 3/4\n","1546/1546 [==============================] - ETA: 0s - loss: 1.6064 - start_loss: 0.8149 - end_loss: 0.7915\n","Epoch 00003: val_loss improved from 1.68213 to 1.67984, saving model to v4-roberta-0.h5\n","1546/1546 [==============================] - 414s 268ms/step - loss: 1.6064 - start_loss: 0.8149 - end_loss: 0.7915 - val_loss: 1.6798 - val_start_loss: 0.8539 - val_end_loss: 0.8259\n","Epoch 4/4\n","1546/1546 [==============================] - ETA: 0s - loss: 1.4923 - start_loss: 0.7491 - end_loss: 0.7431\n","Epoch 00004: val_loss did not improve from 1.67984\n","1546/1546 [==============================] - 413s 267ms/step - loss: 1.4923 - start_loss: 0.7491 - end_loss: 0.7431 - val_loss: 1.7332 - val_start_loss: 0.8865 - val_end_loss: 0.8467\n","Predicting INF...\n","773/773 [==============================] - 126s 163ms/step\n","57\n",">>>> FOLD 1 Jaccard = 0.7716673084207848\n","\n","Predicting OOF...\n","86/86 [==============================] - 14s 161ms/step\n","9\n",">>>> FOLD 1 Jaccard = 0.7058498524590244\n","\n","#########################\n","### FOLD 2\n","#########################\n","<bound method Network.summary of <tensorflow.python.keras.engine.training.Model object at 0x7f581f271f98>>\n","Training model...\n","Epoch 1/4\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","1546/1546 [==============================] - ETA: 0s - loss: 2.3765 - start_loss: 1.1743 - end_loss: 1.2022\n","Epoch 00001: val_loss improved from inf to 1.67582, saving model to v4-roberta-1.h5\n","1546/1546 [==============================] - 417s 270ms/step - loss: 2.3765 - start_loss: 1.1743 - end_loss: 1.2022 - val_loss: 1.6758 - val_start_loss: 0.8556 - val_end_loss: 0.8202\n","Epoch 2/4\n","1546/1546 [==============================] - ETA: 0s - loss: 1.7795 - start_loss: 0.8903 - end_loss: 0.8892\n","Epoch 00002: val_loss improved from 1.67582 to 1.63567, saving model to v4-roberta-1.h5\n","1546/1546 [==============================] - 415s 268ms/step - loss: 1.7795 - start_loss: 0.8903 - end_loss: 0.8892 - val_loss: 1.6357 - val_start_loss: 0.8370 - val_end_loss: 0.7986\n","Epoch 3/4\n","1546/1546 [==============================] - ETA: 0s - loss: 1.6026 - start_loss: 0.8206 - end_loss: 0.7819\n","Epoch 00003: val_loss did not improve from 1.63567\n","1546/1546 [==============================] - 414s 268ms/step - loss: 1.6026 - start_loss: 0.8206 - end_loss: 0.7819 - val_loss: 3.3146 - val_start_loss: 1.6615 - val_end_loss: 1.6531\n","Epoch 4/4\n","1546/1546 [==============================] - ETA: 0s - loss: 1.5228 - start_loss: 0.7712 - end_loss: 0.7516\n","Epoch 00004: val_loss did not improve from 1.63567\n","1546/1546 [==============================] - 415s 268ms/step - loss: 1.5228 - start_loss: 0.7712 - end_loss: 0.7516 - val_loss: 1.6612 - val_start_loss: 0.8431 - val_end_loss: 0.8180\n","Predicting INF...\n","773/773 [==============================] - 126s 163ms/step\n","33\n",">>>> FOLD 2 Jaccard = 0.7642399159022658\n","\n","Predicting OOF...\n","86/86 [==============================] - 14s 161ms/step\n","4\n",">>>> FOLD 2 Jaccard = 0.6971600551047349\n","\n","#########################\n","### FOLD 3\n","#########################\n","<bound method Network.summary of <tensorflow.python.keras.engine.training.Model object at 0x7f59a3acc438>>\n","Training model...\n","Epoch 1/4\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","1546/1546 [==============================] - ETA: 0s - loss: 2.3066 - start_loss: 1.1173 - end_loss: 1.1893\n","Epoch 00001: val_loss improved from inf to 1.61947, saving model to v4-roberta-2.h5\n","1546/1546 [==============================] - 418s 270ms/step - loss: 2.3066 - start_loss: 1.1173 - end_loss: 1.1893 - val_loss: 1.6195 - val_start_loss: 0.8744 - val_end_loss: 0.7451\n","Epoch 2/4\n","1546/1546 [==============================] - ETA: 0s - loss: 1.6913 - start_loss: 0.8663 - end_loss: 0.8250\n","Epoch 00002: val_loss did not improve from 1.61947\n","1546/1546 [==============================] - 414s 268ms/step - loss: 1.6913 - start_loss: 0.8663 - end_loss: 0.8250 - val_loss: 1.6235 - val_start_loss: 0.8672 - val_end_loss: 0.7563\n","Epoch 3/4\n","1546/1546 [==============================] - ETA: 0s - loss: 1.5351 - start_loss: 0.7894 - end_loss: 0.7457\n","Epoch 00003: val_loss improved from 1.61947 to 1.57932, saving model to v4-roberta-2.h5\n","1546/1546 [==============================] - 415s 268ms/step - loss: 1.5351 - start_loss: 0.7894 - end_loss: 0.7457 - val_loss: 1.5793 - val_start_loss: 0.8390 - val_end_loss: 0.7403\n","Epoch 4/4\n","1546/1546 [==============================] - ETA: 0s - loss: 1.4004 - start_loss: 0.7204 - end_loss: 0.6799\n","Epoch 00004: val_loss did not improve from 1.57932\n","1546/1546 [==============================] - 414s 268ms/step - loss: 1.4004 - start_loss: 0.7204 - end_loss: 0.6799 - val_loss: 1.6980 - val_start_loss: 0.8673 - val_end_loss: 0.8307\n","Predicting INF...\n","773/773 [==============================] - 126s 163ms/step\n","35\n",">>>> FOLD 3 Jaccard = 0.7816744026533874\n","\n","Predicting OOF...\n","86/86 [==============================] - 14s 162ms/step\n","7\n",">>>> FOLD 3 Jaccard = 0.7099311444938604\n","\n","#########################\n","### FOLD 4\n","#########################\n","<bound method Network.summary of <tensorflow.python.keras.engine.training.Model object at 0x7f59216d0390>>\n","Training model...\n","Epoch 1/4\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","1546/1546 [==============================] - ETA: 0s - loss: 2.3743 - start_loss: 1.1761 - end_loss: 1.1982\n","Epoch 00001: val_loss improved from inf to 1.77842, saving model to v4-roberta-3.h5\n","1546/1546 [==============================] - 416s 269ms/step - loss: 2.3743 - start_loss: 1.1761 - end_loss: 1.1982 - val_loss: 1.7784 - val_start_loss: 0.8713 - val_end_loss: 0.9072\n","Epoch 2/4\n","1546/1546 [==============================] - ETA: 0s - loss: 1.7951 - start_loss: 0.9075 - end_loss: 0.8876\n","Epoch 00002: val_loss improved from 1.77842 to 1.69798, saving model to v4-roberta-3.h5\n","1546/1546 [==============================] - 414s 268ms/step - loss: 1.7951 - start_loss: 0.9075 - end_loss: 0.8876 - val_loss: 1.6980 - val_start_loss: 0.8522 - val_end_loss: 0.8458\n","Epoch 3/4\n","1546/1546 [==============================] - ETA: 0s - loss: 1.5945 - start_loss: 0.8235 - end_loss: 0.7710\n","Epoch 00003: val_loss did not improve from 1.69798\n","1546/1546 [==============================] - 413s 267ms/step - loss: 1.5945 - start_loss: 0.8235 - end_loss: 0.7710 - val_loss: 1.7147 - val_start_loss: 0.8834 - val_end_loss: 0.8312\n","Epoch 4/4\n","1546/1546 [==============================] - ETA: 0s - loss: 1.4592 - start_loss: 0.7550 - end_loss: 0.7042\n","Epoch 00004: val_loss did not improve from 1.69798\n","1546/1546 [==============================] - 413s 267ms/step - loss: 1.4592 - start_loss: 0.7550 - end_loss: 0.7042 - val_loss: 1.7176 - val_start_loss: 0.8461 - val_end_loss: 0.8715\n","Predicting INF...\n","773/773 [==============================] - 126s 163ms/step\n","22\n",">>>> FOLD 4 Jaccard = 0.7730752644030011\n","\n","Predicting OOF...\n","86/86 [==============================] - 14s 161ms/step\n","3\n",">>>> FOLD 4 Jaccard = 0.6991438050532506\n","\n","#########################\n","### FOLD 5\n","#########################\n","<bound method Network.summary of <tensorflow.python.keras.engine.training.Model object at 0x7f59e431ea90>>\n","Training model...\n","Epoch 1/4\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","1546/1546 [==============================] - ETA: 0s - loss: 2.2594 - start_loss: 1.1274 - end_loss: 1.1321\n","Epoch 00001: val_loss improved from inf to 1.66100, saving model to v4-roberta-4.h5\n","1546/1546 [==============================] - 417s 270ms/step - loss: 2.2594 - start_loss: 1.1274 - end_loss: 1.1321 - val_loss: 1.6610 - val_start_loss: 0.8783 - val_end_loss: 0.7827\n","Epoch 2/4\n","1546/1546 [==============================] - ETA: 0s - loss: 1.6877 - start_loss: 0.8686 - end_loss: 0.8191\n","Epoch 00002: val_loss improved from 1.66100 to 1.60026, saving model to v4-roberta-4.h5\n","1546/1546 [==============================] - 415s 268ms/step - loss: 1.6877 - start_loss: 0.8686 - end_loss: 0.8191 - val_loss: 1.6003 - val_start_loss: 0.8371 - val_end_loss: 0.7632\n","Epoch 3/4\n","1546/1546 [==============================] - ETA: 0s - loss: 1.5227 - start_loss: 0.7861 - end_loss: 0.7366\n","Epoch 00003: val_loss did not improve from 1.60026\n","1546/1546 [==============================] - 414s 268ms/step - loss: 1.5227 - start_loss: 0.7861 - end_loss: 0.7366 - val_loss: 1.6249 - val_start_loss: 0.8569 - val_end_loss: 0.7679\n","Epoch 4/4\n","1546/1546 [==============================] - ETA: 0s - loss: 1.3536 - start_loss: 0.7036 - end_loss: 0.6499\n","Epoch 00004: val_loss did not improve from 1.60026\n","1546/1546 [==============================] - 414s 268ms/step - loss: 1.3536 - start_loss: 0.7036 - end_loss: 0.6499 - val_loss: 1.7389 - val_start_loss: 0.8962 - val_end_loss: 0.8427\n","Predicting INF...\n","773/773 [==============================] - 126s 163ms/step\n","33\n",">>>> FOLD 5 Jaccard = 0.7987956096554599\n","\n","Predicting OOF...\n","86/86 [==============================] - 14s 161ms/step\n","6\n",">>>> FOLD 5 Jaccard = 0.7116922145160869\n","\n","#########################\n","### FOLD 6\n","#########################\n","<bound method Network.summary of <tensorflow.python.keras.engine.training.Model object at 0x7f59e4dae860>>\n","Training model...\n","Epoch 1/4\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","1546/1546 [==============================] - ETA: 0s - loss: 2.2456 - start_loss: 1.1272 - end_loss: 1.1185\n","Epoch 00001: val_loss improved from inf to 1.59513, saving model to v4-roberta-5.h5\n","1546/1546 [==============================] - 417s 270ms/step - loss: 2.2456 - start_loss: 1.1272 - end_loss: 1.1185 - val_loss: 1.5951 - val_start_loss: 0.8250 - val_end_loss: 0.7701\n","Epoch 2/4\n","1546/1546 [==============================] - ETA: 0s - loss: 1.6975 - start_loss: 0.8733 - end_loss: 0.8243\n","Epoch 00002: val_loss improved from 1.59513 to 1.57912, saving model to v4-roberta-5.h5\n","1546/1546 [==============================] - 415s 268ms/step - loss: 1.6975 - start_loss: 0.8733 - end_loss: 0.8243 - val_loss: 1.5791 - val_start_loss: 0.8106 - val_end_loss: 0.7685\n","Epoch 3/4\n","1546/1546 [==============================] - ETA: 0s - loss: 1.5231 - start_loss: 0.7873 - end_loss: 0.7358\n","Epoch 00003: val_loss did not improve from 1.57912\n","1546/1546 [==============================] - 414s 268ms/step - loss: 1.5231 - start_loss: 0.7873 - end_loss: 0.7358 - val_loss: 1.5835 - val_start_loss: 0.8095 - val_end_loss: 0.7740\n","Epoch 4/4\n","1546/1546 [==============================] - ETA: 0s - loss: 1.3553 - start_loss: 0.7029 - end_loss: 0.6524\n","Epoch 00004: val_loss did not improve from 1.57912\n","1546/1546 [==============================] - 414s 268ms/step - loss: 1.3553 - start_loss: 0.7029 - end_loss: 0.6524 - val_loss: 1.6289 - val_start_loss: 0.8494 - val_end_loss: 0.7795\n","Predicting INF...\n","773/773 [==============================] - 126s 163ms/step\n","12\n",">>>> FOLD 6 Jaccard = 0.7997114137030171\n","\n","Predicting OOF...\n","86/86 [==============================] - 14s 162ms/step\n","3\n",">>>> FOLD 6 Jaccard = 0.7118854828832076\n","\n","#########################\n","### FOLD 7\n","#########################\n","<bound method Network.summary of <tensorflow.python.keras.engine.training.Model object at 0x7f59e528e2b0>>\n","Training model...\n","Epoch 1/4\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","1546/1546 [==============================] - ETA: 0s - loss: 2.2783 - start_loss: 1.1405 - end_loss: 1.1377\n","Epoch 00001: val_loss improved from inf to 1.66629, saving model to v4-roberta-6.h5\n","1546/1546 [==============================] - 417s 270ms/step - loss: 2.2783 - start_loss: 1.1405 - end_loss: 1.1377 - val_loss: 1.6663 - val_start_loss: 0.8446 - val_end_loss: 0.8217\n","Epoch 2/4\n","1546/1546 [==============================] - ETA: 0s - loss: 1.6894 - start_loss: 0.8711 - end_loss: 0.8183\n","Epoch 00002: val_loss improved from 1.66629 to 1.59867, saving model to v4-roberta-6.h5\n","1546/1546 [==============================] - 415s 268ms/step - loss: 1.6894 - start_loss: 0.8711 - end_loss: 0.8183 - val_loss: 1.5987 - val_start_loss: 0.8147 - val_end_loss: 0.7840\n","Epoch 3/4\n","1546/1546 [==============================] - ETA: 0s - loss: 1.5325 - start_loss: 0.7932 - end_loss: 0.7393\n","Epoch 00003: val_loss did not improve from 1.59867\n","1546/1546 [==============================] - 414s 267ms/step - loss: 1.5325 - start_loss: 0.7932 - end_loss: 0.7393 - val_loss: 1.6167 - val_start_loss: 0.8202 - val_end_loss: 0.7965\n","Epoch 4/4\n","1546/1546 [==============================] - ETA: 0s - loss: 1.3755 - start_loss: 0.7116 - end_loss: 0.6639\n","Epoch 00004: val_loss did not improve from 1.59867\n","1546/1546 [==============================] - 415s 268ms/step - loss: 1.3755 - start_loss: 0.7116 - end_loss: 0.6639 - val_loss: 1.6748 - val_start_loss: 0.8323 - val_end_loss: 0.8425\n","Predicting INF...\n","773/773 [==============================] - 126s 164ms/step\n","15\n",">>>> FOLD 7 Jaccard = 0.7888579135210567\n","\n","Predicting OOF...\n","86/86 [==============================] - 14s 162ms/step\n","2\n",">>>> FOLD 7 Jaccard = 0.7057724361867738\n","\n","#########################\n","### FOLD 8\n","#########################\n","<bound method Network.summary of <tensorflow.python.keras.engine.training.Model object at 0x7f581ed90ef0>>\n","Training model...\n","Epoch 1/4\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","1546/1546 [==============================] - ETA: 0s - loss: 2.3117 - start_loss: 1.1664 - end_loss: 1.1453\n","Epoch 00001: val_loss improved from inf to 1.72767, saving model to v4-roberta-7.h5\n","1546/1546 [==============================] - 418s 270ms/step - loss: 2.3117 - start_loss: 1.1664 - end_loss: 1.1453 - val_loss: 1.7277 - val_start_loss: 0.8530 - val_end_loss: 0.8746\n","Epoch 2/4\n","1546/1546 [==============================] - ETA: 0s - loss: 1.7003 - start_loss: 0.8829 - end_loss: 0.8174\n","Epoch 00002: val_loss improved from 1.72767 to 1.63601, saving model to v4-roberta-7.h5\n","1546/1546 [==============================] - 416s 269ms/step - loss: 1.7003 - start_loss: 0.8829 - end_loss: 0.8174 - val_loss: 1.6360 - val_start_loss: 0.7938 - val_end_loss: 0.8422\n","Epoch 3/4\n","1546/1546 [==============================] - ETA: 0s - loss: 1.5268 - start_loss: 0.7931 - end_loss: 0.7337\n","Epoch 00003: val_loss improved from 1.63601 to 1.62769, saving model to v4-roberta-7.h5\n","1546/1546 [==============================] - 416s 269ms/step - loss: 1.5268 - start_loss: 0.7931 - end_loss: 0.7337 - val_loss: 1.6277 - val_start_loss: 0.8067 - val_end_loss: 0.8210\n","Epoch 4/4\n","1546/1546 [==============================] - ETA: 0s - loss: 1.3635 - start_loss: 0.7116 - end_loss: 0.6519\n","Epoch 00004: val_loss did not improve from 1.62769\n","1546/1546 [==============================] - 415s 268ms/step - loss: 1.3635 - start_loss: 0.7116 - end_loss: 0.6519 - val_loss: 1.7958 - val_start_loss: 0.8861 - val_end_loss: 0.9097\n","Predicting INF...\n","773/773 [==============================] - 127s 164ms/step\n","12\n",">>>> FOLD 8 Jaccard = 0.7957823344271382\n","\n","Predicting OOF...\n","86/86 [==============================] - 14s 162ms/step\n","4\n",">>>> FOLD 8 Jaccard = 0.7034068620340784\n","\n","#########################\n","### FOLD 9\n","#########################\n","<bound method Network.summary of <tensorflow.python.keras.engine.training.Model object at 0x7f581bde5940>>\n","Training model...\n","Epoch 1/4\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","1546/1546 [==============================] - ETA: 0s - loss: 2.3858 - start_loss: 1.1795 - end_loss: 1.2063\n","Epoch 00001: val_loss improved from inf to 1.74028, saving model to v4-roberta-8.h5\n","1546/1546 [==============================] - 419s 271ms/step - loss: 2.3858 - start_loss: 1.1795 - end_loss: 1.2063 - val_loss: 1.7403 - val_start_loss: 0.8868 - val_end_loss: 0.8534\n","Epoch 2/4\n","1546/1546 [==============================] - ETA: 0s - loss: 1.7421 - start_loss: 0.8927 - end_loss: 0.8493\n","Epoch 00002: val_loss improved from 1.74028 to 1.60486, saving model to v4-roberta-8.h5\n","1546/1546 [==============================] - 416s 269ms/step - loss: 1.7421 - start_loss: 0.8927 - end_loss: 0.8493 - val_loss: 1.6049 - val_start_loss: 0.8458 - val_end_loss: 0.7591\n","Epoch 3/4\n","1546/1546 [==============================] - ETA: 0s - loss: 1.6145 - start_loss: 0.8225 - end_loss: 0.7920\n","Epoch 00003: val_loss did not improve from 1.60486\n","1546/1546 [==============================] - 415s 268ms/step - loss: 1.6145 - start_loss: 0.8225 - end_loss: 0.7920 - val_loss: 1.6363 - val_start_loss: 0.8499 - val_end_loss: 0.7863\n","Epoch 4/4\n","1546/1546 [==============================] - ETA: 0s - loss: 1.4609 - start_loss: 0.7499 - end_loss: 0.7110\n","Epoch 00004: val_loss did not improve from 1.60486\n","1546/1546 [==============================] - 415s 269ms/step - loss: 1.4609 - start_loss: 0.7499 - end_loss: 0.7110 - val_loss: 1.6226 - val_start_loss: 0.8441 - val_end_loss: 0.7785\n","Predicting INF...\n","773/773 [==============================] - 127s 164ms/step\n","42\n",">>>> FOLD 9 Jaccard = 0.7666081449032686\n","\n","Predicting OOF...\n","86/86 [==============================] - 14s 162ms/step\n","6\n",">>>> FOLD 9 Jaccard = 0.7058431036599105\n","\n","#########################\n","### FOLD 10\n","#########################\n","<bound method Network.summary of <tensorflow.python.keras.engine.training.Model object at 0x7f581fff3080>>\n","Training model...\n","Epoch 1/4\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","1546/1546 [==============================] - ETA: 0s - loss: 2.2607 - start_loss: 1.1360 - end_loss: 1.1246\n","Epoch 00001: val_loss improved from inf to 1.76144, saving model to v4-roberta-9.h5\n","1546/1546 [==============================] - 419s 271ms/step - loss: 2.2607 - start_loss: 1.1360 - end_loss: 1.1246 - val_loss: 1.7614 - val_start_loss: 0.9162 - val_end_loss: 0.8453\n","Epoch 2/4\n","1546/1546 [==============================] - ETA: 0s - loss: 1.6909 - start_loss: 0.8688 - end_loss: 0.8221\n","Epoch 00002: val_loss improved from 1.76144 to 1.74585, saving model to v4-roberta-9.h5\n","1546/1546 [==============================] - 417s 270ms/step - loss: 1.6909 - start_loss: 0.8688 - end_loss: 0.8221 - val_loss: 1.7459 - val_start_loss: 0.9163 - val_end_loss: 0.8295\n","Epoch 3/4\n","1546/1546 [==============================] - ETA: 0s - loss: 1.5701 - start_loss: 0.7933 - end_loss: 0.7768\n","Epoch 00003: val_loss did not improve from 1.74585\n","1546/1546 [==============================] - 416s 269ms/step - loss: 1.5701 - start_loss: 0.7933 - end_loss: 0.7768 - val_loss: 3.7386 - val_start_loss: 1.5379 - val_end_loss: 2.2007\n","Epoch 4/4\n","1546/1546 [==============================] - ETA: 0s - loss: 1.5761 - start_loss: 0.7683 - end_loss: 0.8078\n","Epoch 00004: val_loss did not improve from 1.74585\n","1546/1546 [==============================] - 416s 269ms/step - loss: 1.5761 - start_loss: 0.7683 - end_loss: 0.8078 - val_loss: 1.8071 - val_start_loss: 0.9111 - val_end_loss: 0.8960\n","Predicting INF...\n","773/773 [==============================] - 127s 164ms/step\n","282\n",">>>> FOLD 10 Jaccard = 0.751367426289092\n","\n","Predicting OOF...\n","86/86 [==============================] - 14s 162ms/step\n","34\n",">>>> FOLD 10 Jaccard = 0.6825060379403289\n","\n",">>>> OVERALL 10 Fold CV Jaccard = 0.7033190994331258\n","Predicting Test...\n","111/111 [==============================] - 18s 162ms/step\n","Predicting Test...\n","111/111 [==============================] - 18s 162ms/step\n","Predicting Test...\n","111/111 [==============================] - 18s 162ms/step\n","Predicting Test...\n","111/111 [==============================] - 18s 162ms/step\n","Predicting Test...\n","111/111 [==============================] - 18s 161ms/step\n","Predicting Test...\n","111/111 [==============================] - 18s 161ms/step\n","Predicting Test...\n","111/111 [==============================] - 18s 162ms/step\n","Predicting Test...\n","111/111 [==============================] - 18s 161ms/step\n","Predicting Test...\n","111/111 [==============================] - 18s 162ms/step\n","Predicting Test...\n","111/111 [==============================] - 18s 162ms/step\n","8\n","          textID  ...                                                selected_text\n","621   983088f2b4  ...       unlike cierra, i look like poop today. whatevahh, lol.\n","891   66019ddc8f  ...                                  thanks sankar for ur wishes\n","2086  9fbf0d59cc  ...                                             i lafff the rain\n","146   f254748cdb  ...                                                        happy\n","2911  3dd127f4af  ...   hey hunnie how are u?? i miss talkin to u! ty for the f...\n","1241  fcc4f3baa5  ...                                  ahh ok! enjoy! i`ll miss it\n","1533  bab26fdc24  ...                                                          wtf\n","3116  dda6145689  ...                                     cottin with emilyyyyyyyy\n","784   5389177883  ...                    _rain i wanna see her hair hows everyone?\n","2796  4d5a01ceea  ...                                         completely excellent\n","2098  dabac40552  ...                                               super excited.\n","2135  0010bcc0e2  ...                                 recovering from my operation\n","992   472c3e2c41  ...   getting somewhere with my first 'real' kiokudb and cata...\n","828   c392253b42  ...            nope no way in to stop just have to put up wiv it\n","3508  60e5c2c335  ...                                                        bored\n","884   ed298c6e5d  ...                                      oh, that is very nice!!\n","125   410dd99aa3  ...   man im so sad school is ending but then again high scho...\n","3038  b4d1c8c080  ...                                                   can i help\n","323   5aa8a5280f  ...   i knooww & my hot water bottle iss in whangamata withou...\n","3210  a26a75179f  ...                                              haha agreed lol\n","964   2a35e976ea  ...                                       werd. that`s very true\n","594   b70639aec0  ...                                          poor medicated baby\n","3094  00ce730001  ...   unstable broadband and electricity taking toll on my me...\n","1624  4bc4d97503  ...                                                         ****\n","3022  3f09c16e52  ...                           it`s all good. thanks for #ff love\n","\n","[25 rows x 4 columns]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rqTXYi1oadal","colab_type":"code","outputId":"6dedce1a-46e4-4d7c-99f7-66422ca75ab2","executionInfo":{"status":"ok","timestamp":1590471009107,"user_tz":240,"elapsed":732060,"user":{"displayName":"Dechang Chen","photoUrl":"","userId":"17152161433207141669"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["all_selected_text = full_model.predict_test(train, version='v3', model_path=MODEL_PATH, verbose=1)\n","train['pred_selected_text'] = all_selected_text"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Predicting Test...\n","859/859 [==============================] - 138s 160ms/step\n","Predicting Test...\n","859/859 [==============================] - 138s 161ms/step\n","Predicting Test...\n","859/859 [==============================] - 137s 160ms/step\n","Predicting Test...\n","859/859 [==============================] - 137s 160ms/step\n","Predicting Test...\n","859/859 [==============================] - 137s 159ms/step\n","48\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jhArte-va5ya","colab_type":"code","outputId":"6ad4632c-a9d4-4a8d-8a41-fd185126dcbf","executionInfo":{"status":"ok","timestamp":1590471013368,"user_tz":240,"elapsed":1685,"user":{"displayName":"Dechang Chen","photoUrl":"","userId":"17152161433207141669"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["pred_jacs = []\n","for i in range(train.shape[0]):\n","  jac = jaccard(train.loc[i, 'pred_selected_text'], train.loc[i, 'selected_text'])\n","  pred_jacs.append(jac)\n","print(np.mean(pred_jacs))\n","check_pred_jac =pd.DataFrame({'sentiment':train['sentiment'],'pred_jaccard':pred_jacs})\n","check_pred_jac.groupby(['sentiment'])['pred_jaccard'].mean()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0.749140919752366\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["sentiment\n","negative    0.600322\n","neutral     0.977581\n","positive    0.588125\n","Name: pred_jaccard, dtype: float64"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"szokJocQ-P2d","colab_type":"code","colab":{}},"source":["a = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n","b = tf.constant([[10, 20, 30], [40, 50, 60], [70, 80, 90]])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kyeIvNI0-ohT","colab_type":"code","outputId":"51a6116b-ac62-4152-fa83-5604fb132462","executionInfo":{"status":"ok","timestamp":1590346425531,"user_tz":240,"elapsed":1013,"user":{"displayName":"Dechang Chen","photoUrl":"","userId":"17152161433207141669"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["K.mean(K.stack((a, b)), axis=0)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(3, 3), dtype=int32, numpy=\n","array([[ 5, 11, 16],\n","       [22, 27, 33],\n","       [38, 44, 49]], dtype=int32)>"]},"metadata":{"tags":[]},"execution_count":91}]},{"cell_type":"code","metadata":{"id":"raNnMBmwArNE","colab_type":"code","colab":{}},"source":["import torch"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sU1JJD0hAs6L","colab_type":"code","outputId":"8805612f-42bd-45d9-af16-69138fea2737","executionInfo":{"status":"ok","timestamp":1590346185675,"user_tz":240,"elapsed":1316,"user":{"displayName":"Dechang Chen","photoUrl":"","userId":"17152161433207141669"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["a = torch.tensor([[1., 2, 3], [4, 5, 6], [7, 8, 9]])\n","b = torch.tensor([[10., 20, 30], [40, 50, 60], [70, 80, 90]])\n","torch.mean(torch.stack([a, b]), 0)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 5.5000, 11.0000, 16.5000],\n","        [22.0000, 27.5000, 33.0000],\n","        [38.5000, 44.0000, 49.5000]])"]},"metadata":{"tags":[]},"execution_count":76}]},{"cell_type":"code","metadata":{"id":"JoGBeJ5VAs-V","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}