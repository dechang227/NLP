{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"ensemble-models.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"NcADEQ43WkST","colab_type":"code","colab":{}},"source":["#ROOT_PATH = '/content/drive/My Drive/Qishi/NLP/project'\n","ROOT_PATH = '/kaggle'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5L36oCTuWrn3","colab_type":"code","colab":{},"outputId":"3f8209ef-3397-4960-991e-78ce21d46f07"},"source":["import numpy as np\n","import pandas as pd\n","import os\n","import warnings\n","import random\n","import torch \n","from torch import nn\n","import tokenizers\n","from transformers import *\n","\n","import tensorflow as tf\n","import tensorflow.keras.backend as K\n","from tensorflow.keras.layers import Input, Dropout, Conv1D, LeakyReLU, Dense, Flatten, Activation, Reshape, Concatenate, Bidirectional, LSTM\n","from tensorflow.keras.models import Model\n","\n","import gc"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"PjLYhDBqWtPR","colab_type":"code","colab":{}},"source":["warnings.filterwarnings('ignore')\n","\n","TEXT_LEN_BY_WORD = 35"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1q5gJGm3W6Ol","colab_type":"code","colab":{}},"source":["\"\"\"\n","Utility functions\n","\"\"\"\n","\n","def selected_text_by_word_probs(data, start_word_probs, end_word_probs):\n","    data = data.reset_index(drop=True)\n","    selected_text = []\n","    for i in range(data.shape[0]):\n","        start_idx = np.argmax(start_word_probs[i,])\n","        end_idx = np.argmax(end_word_probs[i,])\n","        selected_text.append(\" \".join(data.loc[i, 'text'].split()[start_idx:end_idx+1]))\n","    return selected_text\n","\n","def jaccard(str1, str2): \n","    a = set(str1.lower().split()) \n","    b = set(str2.lower().split())\n","    if (len(a)==0) & (len(b)==0): return 0.5\n","    c = a.intersection(b)\n","    return float(len(c)) / (len(a) + len(b) - len(c))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dTBTwrWvYCkU","colab_type":"text"},"source":["# Pytroch roberta token to word probabilities"]},{"cell_type":"code","metadata":{"id":"0kbui6m0W7Zt","colab_type":"code","colab":{}},"source":["\"\"\"\n","**************************************************************\n","Pytroch token to word probabilities\n","**************************************************************\n","\"\"\"\n","PTROBERTA_MAX_LEN = 128\n","\n","def pt_roberta_prediction_in_word(test, len_by_word=TEXT_LEN_BY_WORD, n_splits=10, model_path='pt-roberta-finetune/'):\n","    class TweetDataset(torch.utils.data.Dataset):\n","        def __init__(self, df, max_len=PTROBERTA_MAX_LEN):\n","            self.df = df\n","            self.max_len = max_len\n","            self.tokenizer = tokenizers.ByteLevelBPETokenizer(\n","                vocab_file=ROOT_PATH+'/input/roberta-base/vocab.json', \n","                merges_file=ROOT_PATH+'/input/roberta-base/merges.txt', \n","                lowercase=True,\n","                add_prefix_space=True)\n","\n","        def __getitem__(self, index):\n","            data = {}\n","            row = self.df.iloc[index]\n","            \n","            ids, masks, tweet, offsets, words_to_tokens_index = self.get_input_data(row)\n","            data['ids'] = ids\n","            data['masks'] = masks\n","            data['tweet'] = tweet\n","            data['offsets'] = offsets\n","            data['words_to_tokens_index'] = words_to_tokens_index\n","\n","            return data\n","\n","        def __len__(self):\n","            return len(self.df)\n","        \n","        def get_input_data(self, row):\n","            tweet = \" \" + \" \".join(row.text.lower().split())\n","            encoding = self.tokenizer.encode(tweet)\n","            sentiment_id = self.tokenizer.encode(row.sentiment).ids\n","            ids = [0] + sentiment_id + [2, 2] + encoding.ids + [2]\n","            offsets = [(0, 0)] * 4 + encoding.offsets + [(0, 0)]\n","\n","            words_to_tokens_index = []\n","            char_position = 0\n","            token_position = 4\n","            for i, word in enumerate(tweet.split()):\n","                words_to_tokens_index.append(token_position)\n","                char_position += 1 + len(word)\n","                while (offsets[token_position][0] < char_position) & (token_position < len(offsets)-1):\n","                    token_position += 1 \n","            \n","            pad_len = self.max_len - len(words_to_tokens_index)\n","            if pad_len > 0:\n","                words_to_tokens_index += [0] * pad_len\n","\n","            pad_len = self.max_len - len(ids)\n","            if pad_len > 0:\n","                ids += [1] * pad_len\n","                offsets += [(0, 0)] * pad_len\n","                    \n","            ids = torch.tensor(ids)\n","            masks = torch.where(ids != 1, torch.tensor(1), torch.tensor(0))\n","            offsets = torch.tensor(offsets)\n","            words_to_tokens_index = torch.tensor(words_to_tokens_index)\n","\n","            return ids, masks, tweet, offsets, words_to_tokens_index\n","\n","\n","\n","    class TweetModel(nn.Module):\n","        def __init__(self):\n","            super(TweetModel, self).__init__()\n","            \n","            config = RobertaConfig.from_pretrained(\n","                ROOT_PATH+'/input/roberta-base/config.json', output_hidden_states=True)    \n","            self.roberta = RobertaModel.from_pretrained(\n","                ROOT_PATH+'/input/roberta-base/pytorch_model.bin', config=config)\n","            self.dropout = nn.Dropout(0.5)\n","            self.fc = nn.Linear(config.hidden_size, 2)\n","            nn.init.normal_(self.fc.weight, std=0.02)\n","            nn.init.normal_(self.fc.bias, 0)\n","\n","        def forward(self, input_ids, attention_mask):\n","            _, _, hs = self.roberta(input_ids, attention_mask)\n","             \n","            x = torch.stack([hs[-1], hs[-2], hs[-3]])\n","            x = torch.max(x, 0)[0]\n","            x = self.dropout(x)\n","            x = self.fc(x)\n","            start_logits, end_logits = x.split(1, dim=-1)\n","            start_logits = start_logits.squeeze(-1)\n","            end_logits = end_logits.squeeze(-1)\n","                    \n","            return start_logits, end_logits\n","\n","\n","    test['text'] = test['text'].astype(str)\n","    test_loader = torch.utils.data.DataLoader(\n","        TweetDataset(test), \n","        batch_size=32, \n","        shuffle=False, \n","        num_workers=2)\n","\n","    start_word_probs = []\n","    end_word_probs=[]\n","    models = []\n","    for fold in range(n_splits):\n","        model = TweetModel()\n","        model.cuda()\n","        model.load_state_dict(torch.load(model_path+'/roberta_finetune_fold%s.pth'%(fold+1)))\n","        model.eval()\n","        models.append(model)\n","        del model\n","        gc.collect()\n","        \n","    for data in test_loader:\n","        ids = data['ids'].cuda()\n","        masks = data['masks'].cuda()\n","        tweet = data['tweet']\n","        offsets = data['offsets'].numpy()\n","        words_to_tokens_index = data['words_to_tokens_index'].numpy()\n","\n","        start_logits = []\n","        end_logits = []\n","        for model in models:\n","            with torch.no_grad():\n","                output = model(ids, masks)\n","                start_logits.append(torch.softmax(output[0], dim=1).cpu().detach().numpy())\n","                end_logits.append(torch.softmax(output[1], dim=1).cpu().detach().numpy())\n","\n","        start_logits = np.mean(start_logits, axis=0)\n","        end_logits = np.mean(end_logits, axis=0)        \n","\n","        for i in range(len(ids)):\n","            num_words = len(tweet[i].split())\n","            pad_len = len_by_word - num_words\n","#             start_word_probs.append(start_logits[i][words_to_tokens_index[i][:num_words]].tolist() + [0]*pad_len)\n","#             end_word_probs.append(end_logits[i][words_to_tokens_index[i][:num_words]].tolist() + [0]*pad_len)\n","            start_word_probs_row = []\n","            end_word_probs_row = []\n","            for j in range(num_words-1):\n","                start_idx = words_to_tokens_index[i, j]\n","                end_idx = words_to_tokens_index[i, j+1]\n","                if start_idx==end_idx:\n","                    end_idx += 1\n","                start_word_probs_row.append(max(start_logits[i][start_idx:end_idx]))\n","                end_word_probs_row.append(max(end_logits[i][start_idx:end_idx]))\n","            start_word_probs_row.append(max(start_logits[i][words_to_tokens_index[i][num_words-1]:]))\n","            end_word_probs_row.append(max(end_logits[i][words_to_tokens_index[i][num_words-1]:]))\n","            \n","            start_word_probs.append(start_word_probs_row + [0]*pad_len)\n","            end_word_probs.append(end_word_probs_row + [0]*pad_len)\n","                \n","    del models, test_loader\n","    gc.collect()\n","    \n","    return (np.array(start_word_probs), np.array(end_word_probs))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E-Vn559iYCkj","colab_type":"text"},"source":["# Pytorch albert QA tokens to words"]},{"cell_type":"code","metadata":{"id":"h4CjlWBEYCkk","colab_type":"code","colab":{}},"source":["\"\"\"\n","**************************************************************\n","Pytroch alberta token to word probabilities\n","**************************************************************\n","\"\"\"\n","PTALBERT_MAX_LEN = 168\n","\n","def pt_albert_prediction_in_word(test, len_by_word=TEXT_LEN_BY_WORD, n_splits=10, model_path='albert-base/'):\n","    class TweetDataset(torch.utils.data.Dataset):\n","        def __init__(self, df, max_len=PTROBERTA_MAX_LEN):\n","            self.df = df\n","            self.max_len = max_len\n","            self.tokenizer = AlbertTokenizer.from_pretrained(ROOT_PATH+'/input/albert-pretrained-models-pytorch/albert-base-v2-spiece.model')\n","\n","        def __getitem__(self, index):\n","            data = {}\n","            row = self.df.iloc[index]\n","            input_dict = self.get_input_data(row)\n","            data['input_ids'] = torch.tensor(input_dict['input_ids'],dtype=torch.long)\n","            data['token_type_ids'] = torch.tensor(input_dict['token_type_ids'],dtype=torch.long)\n","            data['attention_mask'] = torch.tensor(input_dict['attention_mask'],dtype=torch.long)\n","            data['tweet'] = input_dict['tweet']\n","            data['input_text'] = input_dict['input_text']\n","            data['offsets'] = torch.tensor(input_dict['offsets'],dtype=torch.long)\n","            data['sentiment'] = row['sentiment']\n","            data['words_to_tokens_index'] = torch.tensor(input_dict['words_to_tokens_index'],dtype=torch.long)\n","            \n","            return data\n","\n","        def __len__(self):\n","            return len(self.df)\n","        \n","        def get_input_data(self, row):\n","            input_dict = {}\n","            tweet = \" \" + \" \".join(row.text.lower().split())\n","            sentiment = row['sentiment']\n","            question = f\" which words actually lead to the f{sentiment} sentiment description?\"\n","            input_text = question + \" [SEP] \" + tweet\n","            input_text = \" \" + \" \".join(input_text.lower().split())\n","            input_ids = self.tokenizer.encode(input_text)\n","            sep = self.tokenizer.convert_tokens_to_ids(\"[SEP]\")\n","            unk = self.tokenizer.convert_tokens_to_ids(\"<unk>\")\n","            token_type_ids = [0 if i <= input_ids.index(sep) else 1 for i in range(len(input_ids))]\n","            attention_mask = [1] * len(input_ids)\n","            offsets = []\n","            offset_start = 0\n","\n","            for input_id in input_ids[1:-1]:\n","                token = self.tokenizer.convert_ids_to_tokens(input_id)\n","                if input_id == unk: # <unk>\n","                    token = ' '\n","                offset_end = offset_start+len(token)\n","                offsets.append((offset_start, offset_end))\n","                offset_start = offset_end\n","            \n","            words_to_tokens_index = []\n","            token_position = input_ids.index(sep)+1\n","            print(input_text)\n","            print(input_ids)\n","            print(sep)\n","            print(token_position)\n","            for i, word in enumerate(tweet.split()):\n","                words_to_tokens_index.append(token_position)\n","                token_position += len(self.tokenizer.tokenize(word))            \n","            \n","            \n","            pad_len = self.max_len - len(words_to_tokens_index)\n","            if pad_len > 0:\n","                words_to_tokens_index += [0] * pad_len\n","            \n","            \n","            pad_len = self.max_len - len(input_ids)\n","            if pad_len > 0:\n","                input_ids += [0] * pad_len\n","                offsets += [(0, 0)] * pad_len\n","                attention_mask += [0] * pad_len\n","                token_type_ids += [0] * pad_len\n","\n","                \n","            input_dict['input_ids'] = input_ids\n","            input_dict['token_type_ids'] = token_type_ids\n","            input_dict['attention_mask'] = attention_mask\n","            input_dict['offsets'] = offsets\n","            input_dict['input_text'] = input_text  \n","            input_dict['tweet'] = tweet \n","            input_dict['words_to_tokens_index'] = words_to_tokens_index\n","            \n","            return input_dict\n","            \n","\n","    class TweetModel(nn.Module):\n","        def __init__(self):\n","            super(TweetModel, self).__init__()\n","\n","            config = AlbertConfig.from_pretrained(\n","                ROOT_PATH+'/input/albert-pretrained-models-pytorch/albert-base-v2-config.json', output_hidden_states=True)    \n","            self.roberta = AlbertForQuestionAnswering.from_pretrained(\n","                ROOT_PATH+'/input/albert-pretrained-models-pytorch/albert-base-v2-pytorch_model.bin', config=config)\n","            self.dropout = nn.Dropout(0.5)\n","            self.fc = nn.Linear(config.hidden_size, 2)\n","            nn.init.normal_(self.fc.weight, std=0.02)\n","            nn.init.normal_(self.fc.bias, 0)\n","\n","        def forward(self, input_ids, attention_mask):\n","            start_logits, end_logits, hs = self.roberta(input_ids, attention_mask)\n","            return start_logits, end_logits\n","\n","\n","    test['text'] = test['text'].astype(str)\n","    test_loader = torch.utils.data.DataLoader(\n","        TweetDataset(test), \n","        batch_size=32, \n","        shuffle=False, \n","        num_workers=2)\n","\n","    start_word_probs = []\n","    end_word_probs=[]\n","    models = []\n","    for fold in range(n_splits):\n","        model = TweetModel()\n","        model.cuda()\n","        model.load_state_dict(torch.load(model_path+'/albertQA_finetune_fold%s.pth'%(fold+1)))\n","        model.eval()\n","        models.append(model)\n","        del model\n","        gc.collect()\n","        \n","    for data in test_loader:\n","        ids = data['input_ids'].cuda()\n","        masks = data['attention_mask'].cuda()\n","        tweet = data['tweet']\n","        input_text = data['input_text']\n","        offsets = data['offsets'].numpy()\n","        words_to_tokens_index = data['words_to_tokens_index'].numpy()\n","        print(words_to_tokens_index)\n","        \n","        start_logits = []\n","        end_logits = []\n","        for model in models:\n","            with torch.no_grad():\n","                output = model(ids, masks)\n","                start_logits.append(torch.softmax(output[0], dim=1).cpu().detach().numpy())\n","                end_logits.append(torch.softmax(output[1], dim=1).cpu().detach().numpy())\n","\n","        start_logits = np.mean(start_logits, axis=0)\n","        end_logits = np.mean(end_logits, axis=0)        \n","\n","        for i in range(len(ids)):\n","            print(i)\n","            num_words = len(tweet[i].split())\n","            pad_len = len_by_word - num_words\n","#             start_word_probs.append(start_logits[i][words_to_tokens_index[i][:num_words]].tolist() + [0]*pad_len)\n","#             end_word_probs.append(end_logits[i][words_to_tokens_index[i][:num_words]].tolist() + [0]*pad_len)\n","            start_word_probs_row = []\n","            end_word_probs_row = []\n","            for j in range(num_words-1):\n","                #print(j)\n","                start_idx = words_to_tokens_index[i, j]\n","                end_idx = words_to_tokens_index[i, j+1]\n","                if start_idx==end_idx:\n","                    end_idx += 1\n","                start_word_probs_row.append(max(start_logits[i][start_idx:end_idx]))\n","                end_word_probs_row.append(max(end_logits[i][start_idx:end_idx]))\n","            start_word_probs_row.append(max(start_logits[i][words_to_tokens_index[i][num_words-1]:]))\n","            end_word_probs_row.append(max(end_logits[i][words_to_tokens_index[i][num_words-1]:]))\n","            \n","            start_word_probs.append(start_word_probs_row + [0]*pad_len)\n","            end_word_probs.append(end_word_probs_row + [0]*pad_len)\n","                \n","    del models, test_loader\n","    gc.collect()\n","    \n","    return (np.array(start_word_probs), np.array(end_word_probs))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xQPIEhu6YCkv","colab_type":"text"},"source":["# Pytorch XLNET tokens to words"]},{"cell_type":"code","metadata":{"id":"7u8ALY9YYCkw","colab_type":"code","colab":{}},"source":["\"\"\"\n","**************************************************************\n","Pytroch token to word probabilities: xlnet\n","**************************************************************\n","\"\"\"\n","PTXLNET_MAX_LEN = 108\n","TOKENIZER = XLNetTokenizer.from_pretrained(ROOT_PATH+'/input/xlnetbasecased', \n","                                                        remove_space=False,\n","                                                        do_lower_case=True)\n","\n","def pt_xlnet_prediction_in_word(test, len_by_word=TEXT_LEN_BY_WORD, n_splits=10, model_path='xlnet-finetune/'):\n","    def process_data(tweet, sentiment, tokenizer, max_len):\n","        input_ids = np.ones(max_len)\n","        masks = np.zeros(max_len)\n","        token_type_ids = np.zeros(max_len)\n","        sentiment_id = {s:tokenizer.encode(s)[0] for s in ['positive', 'negative', 'neutral']}\n","        text1 = \" \" + \" \".join(tweet.split())\n","        #print(text1)\n","        enc = tokenizer.encode(text1)\n","        s_tok = sentiment_id[sentiment]\n","        input_ids[:len(enc)+5] = [0] + enc + [2,2] + [s_tok] + [2]\n","        masks[:len(enc)+5] = 1\n","\n","        words_to_tokens_index = np.zeros(max_len)\n","        #words_to_tokens_index = []\n","        token_position = 1\n","        for i, word in enumerate(text1.split()):\n","            words_to_tokens_index[i] = token_position\n","            token_position += len(tokenizer.tokenize(word))    \n","        \n","        return {\n","            'ids': input_ids,\n","            'masks': masks,\n","            'token_type_ids': token_type_ids,\n","            'orig_tweet': tweet,\n","            'sentiment': sentiment,\n","            'words_to_tokens_index':words_to_tokens_index\n","        }\n","\n","    class TweetDataset:\n","        def __init__(self, tweet, sentiment):\n","            self.tweet = tweet\n","            self.sentiment = sentiment\n","            self.tokenizer = TOKENIZER\n","            self.max_len = PTXLNET_MAX_LEN\n","\n","        def __len__(self):\n","            return len(self.tweet)\n","\n","        def __getitem__(self, item):\n","            data = process_data(\n","                self.tweet[item], \n","                self.sentiment[item],\n","                self.tokenizer,\n","                self.max_len\n","            )\n","\n","            return {\n","                'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n","                'masks': torch.tensor(data[\"masks\"], dtype=torch.long),\n","                'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n","                'orig_tweet': data[\"orig_tweet\"],\n","                'sentiment': data[\"sentiment\"],\n","                'words_to_tokens_index': torch.tensor(data[\"words_to_tokens_index\"], dtype=torch.long)\n","            }\n","\n","    class TweetModel(XLNetLMHeadModel):\n","        def __init__(self, conf):\n","            super(TweetModel, self).__init__(conf)\n","            self.xlnet = XLNetModel.from_pretrained(ROOT_PATH+\"/input/xlnetbasecased/\",config=conf)\n","            self.drop_out = nn.Dropout(0.3)\n","            self.l0 = nn.Linear(768, 2)\n","            torch.nn.init.normal_(self.l0.weight, std=0.02)\n","            #self.conv1d_1 = nn.Conv1d(PTXLNET_MAX_LEN, 64, 3, padding=1)\n","            #self.conv1d_2 = nn.Conv1d(64, 2, 3, padding=1)\n","\n","        def forward(self, ids, masks, token_type_ids):\n","            _, out = self.xlnet(\n","                ids,\n","                attention_mask=masks,\n","                token_type_ids=token_type_ids\n","            )\n","\n","            out = torch.stack((out[-1], out[-2], out[-3]),dim=0)\n","            out = torch.mean(out, 0)\n","            #out = torch.cat((out[-1], out[-2]), dim=-1)\n","            out = self.drop_out(out)\n","            logits = self.l0(out)\n","            start_logits, end_logits = logits.split(1, dim=-1)\n","            start_logits = start_logits.squeeze(-1)\n","            end_logits = end_logits.squeeze(-1)\n","            return start_logits, end_logits\n","\n","\n","    test['text'] = test['text'].astype(str)\n","    \n","    model_config = XLNetConfig.from_pretrained(ROOT_PATH+\"/input/xlnetbasecased/config.json\")\n","    model_config.output_hidden_states = True\n","    test_dataset = TweetDataset(\n","        tweet=test.text.values,\n","        sentiment=test.sentiment.values)\n","    test_loader = torch.utils.data.DataLoader(\n","            test_dataset, \n","            batch_size=32, \n","            shuffle=False, \n","            num_workers=2)\n","    \n","    start_word_probs = []\n","    end_word_probs=[]\n","    models = []\n","    for fold in range(n_splits):\n","        model = TweetModel(conf=model_config)\n","        model.cuda()\n","        model.load_state_dict(torch.load(model_path+'/XLNetmodel_%s.bin'%(fold)))\n","        model.eval()\n","        models.append(model)\n","        del model\n","        gc.collect()\n","        \n","    for data in test_loader:\n","        ids = data['ids'].cuda()\n","        masks = data['masks'].cuda()\n","        tweet = data['orig_tweet']\n","        sentiment = data['sentiment']\n","        token_type_ids = data['token_type_ids'].cuda()\n","        words_to_tokens_index = data['words_to_tokens_index'].numpy()\n","\n","        start_logits = []\n","        end_logits = []\n","        for model in models:\n","            with torch.no_grad():\n","                output = model(ids, masks, token_type_ids)\n","                start_logits.append(torch.softmax(output[0], dim=1).cpu().detach().numpy())\n","                end_logits.append(torch.softmax(output[1], dim=1).cpu().detach().numpy())\n","\n","        start_logits = np.mean(start_logits, axis=0)\n","        end_logits = np.mean(end_logits, axis=0)        \n","\n","        for i in range(len(ids)):\n","            num_words = len(tweet[i].split())\n","            pad_len = len_by_word - num_words\n","#             start_word_probs.append(start_logits[i][words_to_tokens_index[i][:num_words]].tolist() + [0]*pad_len)\n","#             end_word_probs.append(end_logits[i][words_to_tokens_index[i][:num_words]].tolist() + [0]*pad_len)\n","            start_word_probs_row = []\n","            end_word_probs_row = []\n","            for j in range(num_words-1):\n","                start_idx = words_to_tokens_index[i, j]\n","                end_idx = words_to_tokens_index[i, j+1]\n","                if start_idx==end_idx:\n","                    end_idx += 1\n","                start_word_probs_row.append(max(start_logits[i][start_idx:end_idx]))\n","                end_word_probs_row.append(max(end_logits[i][start_idx:end_idx]))\n","            start_word_probs_row.append(max(start_logits[i][words_to_tokens_index[i][num_words-1]:]))\n","            end_word_probs_row.append(max(end_logits[i][words_to_tokens_index[i][num_words-1]:]))\n","            \n","            start_word_probs.append(start_word_probs_row + [0]*pad_len)\n","            end_word_probs.append(end_word_probs_row + [0]*pad_len)\n","                \n","    del models, test_loader\n","    gc.collect()\n","    \n","    return (np.array(start_word_probs), np.array(end_word_probs))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NpD9RfkHYCk2","colab_type":"text"},"source":["# Tensorflow ROBERTA token to word probabilities"]},{"cell_type":"code","metadata":{"id":"aPakG1_eW_mg","colab_type":"code","colab":{}},"source":["\"\"\"\n","**************************************************************\n","Tensorflow token to word probabilities\n","**************************************************************\n","\"\"\"\n","\n","TFROBERTA_MAX_LEN = 96\n","def tf_roberta_prediction_in_word(test, len_by_word=TEXT_LEN_BY_WORD, n_splits=5, model_path='tf-roberta-finetune/', token_probs=False):\n","\n","    def bert_data_transform(data, max_length=TFROBERTA_MAX_LEN):\n","        '''\n","        Transform data into arrays that BERT understands \n","        '''\n","        tokenizer = tokenizers.ByteLevelBPETokenizer(\n","        vocab_file=ROOT_PATH + '/input/tf-roberta/vocab-roberta-base.json', \n","        merges_file=ROOT_PATH + '/input/tf-roberta/merges-roberta-base.txt', \n","        lowercase=True,\n","        add_prefix_space=True)\n","\n","        sentiment_id = {s:tokenizer.encode(s).ids[0] for s in ['positive', 'negative', 'neutral']}\n","\n","        ct = data.shape[0]\n","        input_ids = np.ones((ct, max_length),dtype='int32')\n","        attention_mask = np.zeros((ct,max_length),dtype='int32')\n","        token_type_ids = np.zeros((ct,max_length),dtype='int32')\n","        words_to_tokens_index = np.zeros((ct,max_length),dtype='int32')\n","\n","        for k in range(ct):\n","            # FIND OVERLAP\n","            text1 = \" \"+\" \".join(data.loc[k,'text'].split())\n","            enc = tokenizer.encode(text1)\n","            s_tok = sentiment_id[data.loc[k,'sentiment']]\n","            input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n","            attention_mask[k,:len(enc.ids)+5] = 1\n","\n","            # ID_OFFSETS\n","            offsets = []; idx=0\n","            for t in enc.ids:\n","                w = tokenizer.decode([t])\n","                offsets.append((idx,idx+len(w)))\n","                idx += len(w)            \n","\n","            char_position = 0\n","            token_position = 1\n","            for i, word in enumerate(text1.split()):\n","                words_to_tokens_index[k, i] = token_position\n","                char_position += 1 + len(word)\n","                while (offsets[token_position-1][0] < char_position) & (token_position < len(offsets)):\n","                    token_position += 1\n","#             print(offsets)\n","#             print(words_to_tokens_index)\n","        return (input_ids, attention_mask, token_type_ids, words_to_tokens_index)\n","        \n","\n","    def build_model(max_length=TFROBERTA_MAX_LEN):\n","        '''\n","        Add layer on top of BERT\n","        '''        \n","        ids = Input((max_length,), dtype=tf.int32)\n","        att = Input((max_length,), dtype=tf.int32)\n","        tok = Input((max_length,), dtype=tf.int32)\n","\n","        \n","        config = RobertaConfig.from_pretrained(ROOT_PATH + '/input/tf-roberta/config-roberta-base.json')\n","        config.output_hidden_states=True\n","        bert_model = TFRobertaModel.from_pretrained(ROOT_PATH + '/input/tf-roberta/pretrained-roberta-base.h5', config=config)\n","        \n","        x = bert_model(ids,attention_mask=att)\n","        \n","        def output_layer(bert_output, name='start', activation='softmax'):\n","            #x_bert = K.max(K.stack(bert_output), axis=0)\n","            x_bert = Dropout(0.1)(bert_output)\n","            x_bert = Conv1D(128, 2,padding='same')(x_bert)\n","            x_bert = LeakyReLU()(x_bert)\n","            x_bert = Conv1D(64, 2, padding='same')(x_bert)\n","            x_bert = Dense(1)(x_bert)                      \n","            x_output = Flatten()(x_bert)\n","            x_output = Activation(activation, name=name)(x_output)\n","             \n","            return x_output\n","        \n","        x1_output = output_layer(x[0], name='start')\n","        x2_output = output_layer(x[0], name='end')\n","        #x1_output = output_layer([x[-1], x[-2], x[-3]], name='start')\n","        #x2_output = output_layer([x[-1], x[-2], x[-3]], name='end')\n","        x_output = output_layer(x[0], name='span', activation='sigmoid')   \n","    \n","        model = Model(inputs=[ids, att], outputs=[x1_output,x2_output, x_output])\n","\n","        return model\n","    \n","        \n","    #test = test.reset_index(drop=True)\n","    input_ids, attention_mask, token_type_ids, words_to_tokens_index = bert_data_transform(test)\n","    \n","    preds_start = np.zeros((input_ids.shape[0],input_ids.shape[1]))\n","    preds_end = np.zeros((input_ids.shape[0],input_ids.shape[1]))\n","\n","    for fold in range(n_splits):\n","        K.clear_session()\n","        model = build_model()\n","        \n","        model.load_weights(model_path+'/v4-roberta-%i.h5'%(fold))\n","        #print('Predicting Test...%s'%fold)\n","        preds = model.predict([input_ids,attention_mask])\n","        preds_start += preds[0]/n_splits\n","        preds_end += preds[1]/n_splits\n","        \n","        del model\n","        gc.collect()\n","        \n","    start_word_probs = np.zeros((input_ids.shape[0],len_by_word))\n","    end_word_probs = np.zeros((input_ids.shape[0],len_by_word))\n","    for i in range(input_ids.shape[0]):\n","        num_words = len(test.loc[i, 'text'].split())\n","        for j in range(num_words-1):\n","            start_idx = words_to_tokens_index[i, j]\n","            end_idx = words_to_tokens_index[i, j+1]\n","            if start_idx==end_idx:\n","                end_idx += 1\n","            start_word_probs[i, j] = max(preds_start[i, start_idx:end_idx])\n","            end_word_probs[i, j] = max(preds_end[i, start_idx:end_idx])\n","        start_word_probs[i, num_words-1] = max(preds_start[i, words_to_tokens_index[i, j]:])\n","        end_word_probs[i, num_words-1] = max(preds_end[i, words_to_tokens_index[i, j]:])\n","    \n","    if token_probs:\n","        return (preds_start, preds_end)\n","    else:\n","        return (start_word_probs, end_word_probs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xUivktcSYCk-","colab_type":"code","colab":{}},"source":["\"\"\"\n","**************************************************************\n","Tensorflow token to word probabilities (ALEX)\n","**************************************************************\n","\"\"\"\n","\n","TFROBERTA_MAX_LEN = 96\n","def tf_roberta_prediction_in_word_2(test, len_by_word=TEXT_LEN_BY_WORD, n_splits=5, model_path='tf-roberta-finetune/', token_probs=False):\n","\n","    def bert_data_transform(data, max_length=TFROBERTA_MAX_LEN):\n","        '''\n","        Transform data into arrays that BERT understands \n","        '''\n","        tokenizer = tokenizers.ByteLevelBPETokenizer(\n","        vocab_file=ROOT_PATH + '/input/tf-roberta/vocab-roberta-base.json', \n","        merges_file=ROOT_PATH + '/input/tf-roberta/merges-roberta-base.txt', \n","        lowercase=True,\n","        add_prefix_space=True)\n","\n","        sentiment_id = {s:tokenizer.encode(s).ids[0] for s in ['positive', 'negative', 'neutral']}\n","\n","        ct = data.shape[0]\n","        input_ids = np.ones((ct, max_length),dtype='int32')\n","        attention_mask = np.zeros((ct,max_length),dtype='int32')\n","        token_type_ids = np.zeros((ct,max_length),dtype='int32')\n","        words_to_tokens_index = np.zeros((ct,max_length),dtype='int32')\n","\n","        for k in range(ct):\n","            # FIND OVERLAP\n","            text1 = \" \"+\" \".join(data.loc[k,'text'].split())\n","            enc = tokenizer.encode(text1)\n","            s_tok = sentiment_id[data.loc[k,'sentiment']]\n","            input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n","            attention_mask[k,:len(enc.ids)+5] = 1\n","\n","            # ID_OFFSETS\n","            offsets = []; idx=0\n","            for t in enc.ids:\n","                w = tokenizer.decode([t])\n","                offsets.append((idx,idx+len(w)))\n","                idx += len(w)            \n","\n","            char_position = 0\n","            token_position = 1\n","            for i, word in enumerate(text1.split()):\n","                words_to_tokens_index[k, i] = token_position\n","                char_position += 1 + len(word)\n","                while (offsets[token_position-1][0] < char_position) & (token_position < len(offsets)):\n","                    token_position += 1\n","#             print(offsets)\n","#             print(words_to_tokens_index)\n","        return (input_ids, attention_mask, token_type_ids, words_to_tokens_index)\n","        \n","\n","    def build_model(max_length=TFROBERTA_MAX_LEN):\n","        '''\n","        Add layer on top of BERT\n","        '''        \n","        ids = Input((max_length,), dtype=tf.int32)\n","        att = Input((max_length,), dtype=tf.int32)\n","        tok = Input((max_length,), dtype=tf.int32)\n","\n","        \n","        config = RobertaConfig.from_pretrained(ROOT_PATH + '/input/tf-roberta/config-roberta-base.json')\n","        config.output_hidden_states=True\n","        bert_model = TFRobertaModel.from_pretrained(ROOT_PATH + '/input/tf-roberta/pretrained-roberta-base.h5', config=config)\n","        \n","        \n","        x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n","    \n","        x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n","        x1 = tf.keras.layers.Conv1D(1,1)(x1)\n","        x1 = tf.keras.layers.Flatten()(x1)\n","        x1 = tf.keras.layers.Activation('softmax')(x1)\n","    \n","        x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n","        x2 = tf.keras.layers.Conv1D(1,1)(x2)\n","        x2 = tf.keras.layers.Flatten()(x2)\n","        x2 = tf.keras.layers.Activation('softmax')(x2)\n","    \n","        model = Model(inputs=[ids, att, tok], outputs=[x1, x2])\n","\n","        return model\n","    \n","        \n","    #test = test.reset_index(drop=True)\n","    input_ids, attention_mask, token_type_ids, words_to_tokens_index = bert_data_transform(test)\n","    \n","    preds_start = np.zeros((input_ids.shape[0],input_ids.shape[1]))\n","    preds_end = np.zeros((input_ids.shape[0],input_ids.shape[1]))\n","\n","    for fold in range(n_splits):\n","        K.clear_session()\n","        model = build_model()\n","        \n","        model.load_weights(model_path+'/reg_0_05-roberta-%i.h5'%(fold))\n","        #print('Predicting Test...%s'%fold)\n","        preds = model.predict([input_ids,attention_mask, token_type_ids])\n","        preds_start += preds[0]/n_splits\n","        preds_end += preds[1]/n_splits\n","        \n","        del model\n","        gc.collect()\n","        \n","    start_word_probs = np.zeros((input_ids.shape[0],len_by_word))\n","    end_word_probs = np.zeros((input_ids.shape[0],len_by_word))\n","    for i in range(input_ids.shape[0]):\n","        num_words = len(test.loc[i, 'text'].split())\n","        for j in range(num_words-1):\n","            start_idx = words_to_tokens_index[i, j]\n","            end_idx = words_to_tokens_index[i, j+1]\n","            if start_idx==end_idx:\n","                end_idx += 1\n","            start_word_probs[i, j] = max(preds_start[i, start_idx:end_idx])\n","            end_word_probs[i, j] = max(preds_end[i, start_idx:end_idx])\n","        start_word_probs[i, num_words-1] = max(preds_start[i, words_to_tokens_index[i, j]:])\n","        end_word_probs[i, num_words-1] = max(preds_end[i, words_to_tokens_index[i, j]:])\n","            \n","    if token_probs:\n","        return (preds_start, preds_end)\n","    else:\n","        return (start_word_probs, end_word_probs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XKrHifz_YClD","colab_type":"code","colab":{}},"source":["def is_connect(l):\n","    return l in {\"'\", \"-\", '_'}\n","\n","def extend_select(tokenizer, predicted_start, predicted_end, original_text, extend_threshold  = 0):\n","\n","    a = predicted_start\n","    b = predicted_end\n","\n","    enc = tokenizer.encode(original_text)\n","    res = tokenizer.decode(enc.ids[a-1:b])\n","\n","    if b < len(enc.ids):\n","        next_wd = tokenizer.decode(enc.ids[b:(b+1)])\n","        #if next_wd != '' and next_wd[0] != ' ':\n","        if next_wd != '' and next_wd[0] != ' ' and ((next_wd[0].isalnum() and res[-1].isalnum()) or (is_connect(next_wd[0]) or (is_connect(res[-1])))):\n","            last_wd = tokenizer.decode(enc.ids[(b-1):b])\n","\n","            if len(last_wd)/(len(last_wd) + len(next_wd)) > extend_threshold:\n","                res = res + next_wd\n","            else:\n","                res = tokenizer.decode(enc.ids[a-1:b-1])\n","\n","    if a > 1 and res[0].isalnum() and tokenizer.decode(enc.ids[(a-2):(a-1)])[-1] == '#':\n","        res = '#' + res\n","        return res\n","\n","    if a > 1:\n","        prev_wd = tokenizer.decode(enc.ids[(a-2):(a-1)])\n","\n","        if prev_wd != '' and prev_wd[-1] != ' ' and res[0] != ' ' and ((prev_wd[-1].isalnum() and res[0].isalnum()) or (is_connect(prev_wd[-1]) or (is_connect(res[0])))):\n","\n","            first_wd = tokenizer.decode(enc.ids[(a-1):a])\n","\n","            if len(first_wd)/(len(first_wd) + len(prev_wd)) > extend_threshold:\n","                res = prev_wd + res\n","            else:\n","                res = tokenizer.decode(enc.ids[a:b-1])\n","\n","    return res\n","\n","def predict_decode(tokenizer, text_data, vec_start, vec_end, vec_idx):\n","    all_selected_text = []\n","    for k in vec_idx:\n","        a = np.argmax(vec_start[k,])\n","        b = np.argmax(vec_end[k,])\n","        if a>b: \n","            st = text_data.loc[k,'text']\n","        else:\n","            text1 = \" \"+\" \".join(text_data.loc[k,'text'].split())\n","            enc = tokenizer.encode(text1)\n","            #st = tokenizer.decode(enc.ids[a-1:b])\n","            st = extend_select(tokenizer, a, b, text1)\n","        all_selected_text.append(st)\n","    return all_selected_text"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"54KJIZAeXgFw","colab_type":"text"},"source":["# Validate on train fold"]},{"cell_type":"code","metadata":{"id":"LtFczcBxXjfc","colab_type":"code","colab":{}},"source":["# train_fd = pd.read_csv(ROOT_PATH+'/input/tweet-train-folds-v2/train_folds.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gu7JZYjBYClP","colab_type":"code","colab":{}},"source":["# train_fd.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E4RrF5FgYClT","colab_type":"code","colab":{}},"source":["# test = train_fd.sample(3, random_state=777).reset_index(drop=True)\n","# test\n","#pt_roberta_start_word_probs, pt_roberta_end_word_probs = pt_roberta_prediction_in_word(test, len_by_word=TEXT_LEN_BY_WORD, n_splits=1, model_path=ROOT_PATH+'/input/ptrobertafinetune716')\n","#tf_roberta_start_word_probs, tf_roberta_end_word_probs = tf_roberta_prediction_in_word_2(test, len_by_word=TEXT_LEN_BY_WORD, n_splits=1, model_path=ROOT_PATH+'/input/alex-roberta')\n","#pt_albert_start_word_probs, pt_albert_end_word_probs = pt_albert_prediction_in_word(test, len_by_word=TEXT_LEN_BY_WORD, n_splits=1, model_path=ROOT_PATH+'/input/albert-base')\n","#pt_xlnet_start_word_probs, pt_xlnet_end_word_probs = pt_xlnet_prediction_in_word(test, len_by_word=TEXT_LEN_BY_WORD, n_splits=1, model_path=ROOT_PATH+'/input/xlnet-base-models')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TvLv7W5JYClW","colab_type":"code","colab":{}},"source":["# start_1, end_1 = tf_roberta_prediction_in_word_2(test, len_by_word=TEXT_LEN_BY_WORD, n_splits=1, model_path=ROOT_PATH+'/input/alex-roberta', token_probs=True)\n","# start_2, end_2 = tf_roberta_prediction_in_word(test, len_by_word=TEXT_LEN_BY_WORD, n_splits=1, model_path=ROOT_PATH+'/input/tfrobertafinetuneaddspan711', token_probs=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TGaXaNAUYClb","colab_type":"code","colab":{}},"source":["# r=0.5\n","# start_probs = np.zeros((test.shape[0],TFROBERTA_MAX_LEN),dtype=np.float32)\n","# end_probs = np.zeros((test.shape[0],TFROBERTA_MAX_LEN),dtype=np.float32)\n","\n","# start_probs += r*start_1\n","# start_probs += (1-r)*start_2\n","\n","# end_probs += r*end_1\n","# end_probs += (1-r)*end_2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ilJ0TCTwYClf","colab_type":"code","colab":{}},"source":["# tokenizer = tokenizers.ByteLevelBPETokenizer(\n","#         vocab_file=ROOT_PATH + '/input/tf-roberta/vocab-roberta-base.json', \n","#         merges_file=ROOT_PATH + '/input/tf-roberta/merges-roberta-base.txt', \n","#         lowercase=True,\n","#         add_prefix_space=True)\n","\n","# predict_decode(tokenizer, test, start_probs, end_probs, test.index)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jwmooF81YClk","colab_type":"code","colab":{}},"source":["# train = train_fd[train_fd['kfold']==0].copy().reset_index(drop=True)    \n","# train = train.loc[27:29, ].reset_index(drop=True)\n","# pt_albert_start_word_probs, pt_albert_end_word_probs = pt_albert_prediction_in_word(train, len_by_word=TEXT_LEN_BY_WORD, n_splits=1, model_path=ROOT_PATH+'/input/albert-base')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hs9FOutaYClr","colab_type":"code","colab":{}},"source":["#train.loc[1, 'text']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nWP-c48Rcx23","colab_type":"code","colab":{}},"source":["# # # #for k in range(5):\n","# k=0\n","# print('>>>> FOLD %i >>>>>>>'%k)\n","\n","# train = train_fd[train_fd['kfold']==k].copy().reset_index(drop=True)    \n","# start_word_probs_1, end_word_probs_1 = tf_roberta_prediction_in_word_2(train, len_by_word=TEXT_LEN_BY_WORD, n_splits=1, model_path=ROOT_PATH+'/input/alex-roberta')\n","# start_word_probs_2, end_word_probs_2 = tf_roberta_prediction_in_word(train, len_by_word=TEXT_LEN_BY_WORD, n_splits=5, model_path=ROOT_PATH+'/input/tfrobertafinetuneaddspan711')\n","\n","# selelcted_text_1 = selected_text_by_word_probs(train, start_word_probs_1, end_word_probs_1)\n","# selelcted_text_2 = selected_text_by_word_probs(train, start_word_probs_2, end_word_probs_2)\n","\n","# start_word_probs = np.zeros((train.shape[0],TEXT_LEN_BY_WORD),dtype=np.float32)\n","# end_word_probs = np.zeros((train.shape[0],TEXT_LEN_BY_WORD),dtype=np.float32)\n","\n","# start_word_probs += r*start_word_probs_1\n","# start_word_probs += (1-r)*start_word_probs_2\n","\n","# end_word_probs += r*end_word_probs_1\n","# end_word_probs += (1-r)*end_word_probs_2\n","\n","# all_selected_text = selected_text_by_word_probs(train, start_word_probs, end_word_probs)\n","\n","# jac_1 = []\n","# jac_2 = []\n","# ensemble_jac = []\n","# for i in range(train.shape[0]):\n","#     jac_1.append(jaccard(train.loc[i, 'selected_text'], selelcted_text_1[i]))\n","#     jac_2.append(jaccard(train.loc[i, 'selected_text'], selelcted_text_2[i]))\n","#     ensemble_jac.append(jaccard(train.loc[i, 'selected_text'], all_selected_text[i]))\n","\n","# print('>>>> jaccard 1 =', np.mean(jac_1))\n","# print('>>>> jaccard 2 =', np.mean(jac_2))\n","# print('>>>> ensemble jaccard =', np.mean(ensemble_jac))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xdTqqaKJYCl1","colab_type":"code","colab":{}},"source":["# tokenizer = tokenizers.ByteLevelBPETokenizer(\n","#         vocab_file=ROOT_PATH + '/input/tf-roberta/vocab-roberta-base.json', \n","#         merges_file=ROOT_PATH + '/input/tf-roberta/merges-roberta-base.txt', \n","#         lowercase=True,\n","#         add_prefix_space=True)\n","\n","# # # #for k in range(5):\n","# k=0\n","# print('>>>> FOLD %i >>>>>>>'%k)\n","\n","# train = train_fd[train_fd['kfold']==k].copy().reset_index(drop=True)    \n","# start_1, end_1 = tf_roberta_prediction_in_word_2(train, len_by_word=TEXT_LEN_BY_WORD, n_splits=1, model_path=ROOT_PATH+'/input/alex-roberta', token_probs=True)\n","# start_2, end_2 = tf_roberta_prediction_in_word(train, len_by_word=TEXT_LEN_BY_WORD, n_splits=5, model_path=ROOT_PATH+'/input/tfrobertafinetuneaddspan711', token_probs=True)\n","\n","# selelcted_text_1 = predict_decode(tokenizer,train, start_1, end_1, train.index)\n","# selelcted_text_2 = predict_decode(tokenizer,train, start_2, end_2, train.index)\n","\n","# jac_1 = []\n","# jac_2 = []\n","\n","# for i in range(train.shape[0]):\n","#     jac_1.append(jaccard(train.loc[i, 'selected_text'], selelcted_text_1[i]))\n","#     jac_2.append(jaccard(train.loc[i, 'selected_text'], selelcted_text_2[i]))\n","    \n","# print('>>>> jaccard 1 =', np.mean(jac_1))\n","# print('>>>> jaccard 2 =', np.mean(jac_2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j9We8zzZYCl3","colab_type":"code","colab":{}},"source":["# r=0.1594"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zN7j--eqYCl9","colab_type":"code","colab":{}},"source":["# start_probs = np.zeros((train.shape[0],TFROBERTA_MAX_LEN),dtype=np.float32)\n","# end_probs = np.zeros((train.shape[0],TFROBERTA_MAX_LEN),dtype=np.float32)\n","\n","# start_probs += r*start_1\n","# start_probs += (1-r)*start_2\n","\n","# end_probs += r*end_1\n","# end_probs += (1-r)*end_2\n","\n","# all_selected_text = predict_decode(tokenizer,train, start_probs, end_probs, train.index)\n","\n","# ensemble_jac = []\n","# for i in range(train.shape[0]):\n","#     ensemble_jac.append(jaccard(train.loc[i, 'selected_text'], all_selected_text[i]))\n","\n","# print('>>>> ensemble jaccard =', np.mean(ensemble_jac))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wpz2QdCaXZVN","colab_type":"text"},"source":["# Submission"]},{"cell_type":"code","metadata":{"id":"tWAmxj_SYCmI","colab_type":"code","colab":{}},"source":["r=0.5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ISC28PEMYCmV","colab_type":"code","colab":{}},"source":["tokenizer = tokenizers.ByteLevelBPETokenizer(\n","        vocab_file=ROOT_PATH + '/input/tf-roberta/vocab-roberta-base.json', \n","        merges_file=ROOT_PATH + '/input/tf-roberta/merges-roberta-base.txt', \n","        lowercase=True,\n","        add_prefix_space=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WRXyzPsLXdUA","colab_type":"code","colab":{}},"source":["# \"\"\"\n","# **************************************************************\n","# Ensemble\n","# **************************************************************\n","# \"\"\"\n","# test = pd.read_csv(ROOT_PATH+'/input/tweet-sentiment-extraction/test.csv')\n","# #test = test.sample(3, random_state=SEED).reset_index(drop=True)\n","\n","# start_1, end_1 = tf_roberta_prediction_in_word_2(test, len_by_word=TEXT_LEN_BY_WORD, n_splits=1, model_path=ROOT_PATH+'/input/alex-roberta', token_probs=True)\n","# start_2, end_2 = tf_roberta_prediction_in_word(test, len_by_word=TEXT_LEN_BY_WORD, n_splits=5, model_path=ROOT_PATH+'/input/tfrobertafinetuneaddspan711', token_probs=True)\n","\n","# start_word_probs = np.zeros((test.shape[0],TEXT_LEN_BY_WORD),dtype=np.float32)\n","# end_word_probs = np.zeros((test.shape[0],TEXT_LEN_BY_WORD),dtype=np.float32)\n","\n","# start_word_probs += r*start_word_probs_1\n","# start_word_probs += (1-r)*start_word_probs_2\n","\n","# end_word_probs += r*end_word_probs_1\n","# end_word_probs += (1-r)*end_word_probs_2\n","\n","\n","# all_selected_text = predict_decode(tokenizer,test, start_probs, end_probs, test.index)\n","\n","# test['selected_text'] = all_selected_text\n","# test[['textID','selected_text']].to_csv('submission.csv',index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Rv8ZNeAYCmb","colab_type":"code","colab":{}},"source":["# \"\"\"\n","# **************************************************************\n","# Ensemble\n","# **************************************************************\n","# \"\"\"\n","# test = pd.read_csv(ROOT_PATH+'/input/tweet-sentiment-extraction/test.csv')\n","# #test = test.sample(3, random_state=SEED).reset_index(drop=True)\n","\n","# start_1, end_1 = tf_roberta_prediction_in_word_2(test, len_by_word=TEXT_LEN_BY_WORD, n_splits=1, model_path=ROOT_PATH+'/input/alex-roberta', token_probs=True)\n","# start_2, end_2 = tf_roberta_prediction_in_word(test, len_by_word=TEXT_LEN_BY_WORD, n_splits=5, model_path=ROOT_PATH+'/input/tfrobertafinetuneaddspan711', token_probs=True)\n","\n","# start_probs = np.zeros((test.shape[0],TFROBERTA_MAX_LEN),dtype=np.float32)\n","# end_probs = np.zeros((test.shape[0],TFROBERTA_MAX_LEN),dtype=np.float32)\n","\n","# start_probs += r*start_1\n","# start_probs += (1-r)*start_2\n","\n","# end_probs += r*end_1\n","# end_probs += (1-r)*end_2\n","\n","\n","# all_selected_text = predict_decode(tokenizer,test, start_probs, end_probs, test.index)\n","\n","# test['selected_text'] = all_selected_text\n","# test[['textID','selected_text']].to_csv('submission.csv',index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lMuOI0pfYCmf","colab_type":"code","colab":{}},"source":["\"\"\"\n","**************************************************************\n","Ensemble\n","**************************************************************\n","\"\"\"\n","test = pd.read_csv(ROOT_PATH+'/input/tweet-sentiment-extraction/test.csv')\n","#test = test.sample(3, random_state=SEED).reset_index(drop=True)\n","\n","start_1, end_1 = tf_roberta_prediction_in_word_2(test, len_by_word=TEXT_LEN_BY_WORD, n_splits=8, model_path=ROOT_PATH+'/input/reg-0-05', token_probs=True)\n","start_2, end_2 = tf_roberta_prediction_in_word(test, len_by_word=TEXT_LEN_BY_WORD, n_splits=5, model_path=ROOT_PATH+'/input/tfrobertafinetuneaddspan711', token_probs=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HJWwtCxEYCmj","colab_type":"code","colab":{}},"source":["start_probs = np.zeros((test.shape[0],TFROBERTA_MAX_LEN),dtype=np.float32)\n","end_probs = np.zeros((test.shape[0],TFROBERTA_MAX_LEN),dtype=np.float32)\n","\n","\n","for i in range(test.shape[0]):\n","    if 0.8*np.var(start_1[i,:]) >= np.var(start_2[i,:]): \n","        start_probs = start_1\n","    else:\n","        start_probs = start_2\n","\n","    if 0.8*np.var(end_1[i,:]) >= np.var(end_2[i,:]): \n","        end_probs = end_1\n","    else:\n","        end_probs = end_2\n","\n","\n","all_selected_text = predict_decode(tokenizer,test, start_probs, end_probs, test.index)\n","\n","test['selected_text'] = all_selected_text\n","test[['textID','selected_text']].to_csv('submission.csv',index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AbljBi8aYCmm","colab_type":"code","colab":{}},"source":["# probs_var = np.zeros((test.shape[0],4),dtype=np.float32)\n","# for i in range(test.shape[0]):\n","#     probs_var[i, 0] = np.var(start_1[i,])\n","#     probs_var[i, 1] = np.var(start_2[i,])\n","#     probs_var[i, 2] = np.var(end_1[i,])\n","#     probs_var[i, 3] = np.var(end_2[i,])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d3WXv9hHYCmo","colab_type":"code","colab":{}},"source":["# start_1[:5,]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SDNmiJ-JYCmq","colab_type":"code","colab":{}},"source":["# probs_var[:15,]"],"execution_count":null,"outputs":[]}]}