{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"name":"roberta_fine_tuning_last_try_add_span.ipynb","provenance":[{"file_id":"1I_8GG0vAyQsqJiHGQHvmKQ9We57-EJJR","timestamp":1590617913620},{"file_id":"1dC1AY08Em9vXu8OfCV1YK9BJQ7RmZp47","timestamp":1590466272254}],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"20aDfXGcAGq4","colab_type":"text"},"source":["**Add features on top of BERT fine tunning, see reference**\n","\n","https://datascience.stackexchange.com/questions/54888/how-can-i-add-custom-numerical-features-for-training-to-bert-fine-tuning\n","\n","https://www.pyimagesearch.com/2019/02/04/keras-multiple-inputs-and-mixed-data/\n","\n","Binary_crossentropy: https://gombru.github.io/2018/05/23/cross_entropy_loss/"]},{"cell_type":"code","metadata":{"id":"F-LCG6ynAmjv","colab_type":"code","outputId":"0fa400e9-3bce-4003-9a84-a20b02b7c2e2","executionInfo":{"status":"ok","timestamp":1590773419582,"user_tz":240,"elapsed":17398,"user":{"displayName":"Dechang Chen","photoUrl":"","userId":"17152161433207141669"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VOM0pUojDLQM","colab_type":"code","outputId":"21c94dab-ebb2-4a27-abae-d264ecd3dc9f","executionInfo":{"status":"ok","timestamp":1590773447002,"user_tz":240,"elapsed":6940,"user":{"displayName":"Dechang Chen","photoUrl":"","userId":"17152161433207141669"}},"colab":{"base_uri":"https://localhost:8080/","height":561}},"source":["!pip install transformers"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/b5/ac41e3e95205ebf53439e4dd087c58e9fd371fd8e3724f2b9b4cdb8282e5/transformers-2.10.0-py3-none-any.whl (660kB)\n","\u001b[K     |████████████████████████████████| 665kB 4.8MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 22.7MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 30.8MB/s \n","\u001b[?25hCollecting tokenizers==0.7.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 61.5MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=8a91c61de2bb0cb4359037c4599111be6032edba1bf6253cc4c6a5c5bfb19f67\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.10.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ettAIDi-CADj","colab_type":"code","colab":{}},"source":["ROOT_PATH = '/content/drive/My Drive/Qishi/NLP/project'\n","# ROOT_PATH = '/kaggle'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"id":"5AlHAPzoAGq6","colab_type":"code","outputId":"bbf02d2d-0d51-4a43-8e0a-ee7d7bbfb03a","executionInfo":{"status":"ok","timestamp":1590773714923,"user_tz":240,"elapsed":636,"user":{"displayName":"Dechang Chen","photoUrl":"","userId":"17152161433207141669"}},"colab":{"base_uri":"https://localhost:8080/","height":255}},"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk(ROOT_PATH+'/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Qishi/NLP/project/input/tweet-sentiment-extraction/train.csv\n","/content/drive/My Drive/Qishi/NLP/project/input/tweet-sentiment-extraction/sample_submission.csv\n","/content/drive/My Drive/Qishi/NLP/project/input/tweet-sentiment-extraction/test.csv\n","/content/drive/My Drive/Qishi/NLP/project/input/tfrobert/glove_6B_100d_top100k.csv\n","/content/drive/My Drive/Qishi/NLP/project/input/tfrobert/roberta-base-vocab.json\n","/content/drive/My Drive/Qishi/NLP/project/input/tfrobert/roberta-base-merges.txt\n","/content/drive/My Drive/Qishi/NLP/project/input/tfrobert/pretrained-roberta-base.h5\n","/content/drive/My Drive/Qishi/NLP/project/input/tfrobert/roberta-base-tf_model.h5\n","/content/drive/My Drive/Qishi/NLP/project/input/tfrobert/config-roberta-base.json\n","/content/drive/My Drive/Qishi/NLP/project/input/tfrobert/roberta-base-config.json\n","/content/drive/My Drive/Qishi/NLP/project/input/roberta-base/pytorch_model.bin\n","/content/drive/My Drive/Qishi/NLP/project/input/roberta-base/config.json\n","/content/drive/My Drive/Qishi/NLP/project/input/roberta-base/merges.txt\n","/content/drive/My Drive/Qishi/NLP/project/input/roberta-base/vocab.json\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"id":"ilDKj7ZfAGq_","colab_type":"code","outputId":"1e35359d-6517-48e5-8a00-d089943fa626","executionInfo":{"status":"ok","timestamp":1590773714924,"user_tz":240,"elapsed":627,"user":{"displayName":"Dechang Chen","photoUrl":"","userId":"17152161433207141669"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import pandas as pd, numpy as np\n","import tensorflow as tf\n","import tensorflow.keras.backend as K\n","from sklearn.model_selection import StratifiedKFold, ShuffleSplit\n","from transformers import *\n","import tokenizers\n","import gc\n","from tensorflow.python.framework import ops\n","print('TF version',tf.__version__)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["TF version 2.2.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"trusted":true,"id":"gL1GtYaXAGrE","colab_type":"code","colab":{}},"source":["from tensorflow.keras.layers import Input, Dropout, Conv1D, LeakyReLU, Dense, Flatten, Activation, Reshape, Concatenate, Bidirectional, LSTM\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","\n","learning_rate_fn = tf.keras.optimizers.schedules.PolynomialDecay(\n","    initial_learning_rate=2e-5,\n","    decay_steps=10000,\n","    end_learning_rate=1e-5,\n","    power=1)\n","\n","def scheduler(epoch):\n","    return 3e-5 * 0.2**epoch\n","\n","def jaccard(str1, str2): \n","    a = set(str1.lower().split()) \n","    b = set(str2.lower().split())\n","    if (len(a)==0) & (len(b)==0): return 0.5\n","    c = a.intersection(b)\n","    return float(len(c)) / (len(a) + len(b) - len(c))\n","\n","\n","class FineTuneBertForQA:\n","    def __init__(self, tokenizer, config_file, bert_model_file, max_length, n_splits, loss_fn='categorical_crossentropy', loss_weights=None, separated_output=True, seed=777):\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","        self.sentiment_id = {s:self.tokenizer.encode(s).ids[0] for s in ['positive', 'negative', 'neutral']}\n","        self.config_file = config_file\n","        self.bert_model_file = bert_model_file\n","        self.n_splits = n_splits\n","        self.loss_fn = loss_fn\n","        self.loss_weights = loss_weights\n","        self.separated_output = separated_output\n","        self.seed = seed\n","\n","    def bert_data_transform(self, data, train=True):\n","        '''\n","        Transform data into arrays that BERT understands \n","        '''\n","        ct = data.shape[0]\n","        input_ids = np.ones((ct,self.max_length),dtype='int32')\n","        attention_mask = np.zeros((ct,self.max_length),dtype='int32')\n","        #token_type_ids = np.zeros((ct,self.max_length),dtype='int32')\n","        if train:\n","            if self.separated_output:\n","                start_tokens = np.zeros((ct,self.max_length),dtype='int32')\n","                end_tokens = np.zeros((ct,self.max_length),dtype='int32')\n","                span_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n","            else:\n","                span_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n","\n","        for k in range(ct):\n","            # FIND OVERLAP\n","            text1 = \" \"+\" \".join(data.loc[k,'text'].split())\n","            enc = self.tokenizer.encode(text1)\n","            s_tok = self.sentiment_id[data.loc[k,'sentiment']]\n","            input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n","            attention_mask[k,:len(enc.ids)+5] = 1\n","            \n","            if train:\n","                text2 = \" \".join(data.loc[k,'selected_text'].split())\n","                idx = text1.find(text2)\n","                chars = np.zeros((len(text1)))\n","                chars[idx:idx+len(text2)]=1\n","                if text1[idx-1]==' ': chars[idx-1] = 1 \n","                # ID_OFFSETS\n","                offsets = []; idx=0\n","                for t in enc.ids:\n","                    w = self.tokenizer.decode([t])\n","                    offsets.append((idx,idx+len(w)))\n","                    idx += len(w)\n","\n","                # START END TOKENS\n","                toks = []\n","                for i,(a,b) in enumerate(offsets):\n","                    sm = np.sum(chars[a:b])\n","                    if sm>0: toks.append(i) \n","\n","                if len(toks)>0:\n","                    if self.separated_output:\n","                        start_tokens[k,toks[0]+1] = 1\n","                        end_tokens[k,toks[-1]+1] = 1\n","                        for i in range(toks[0], toks[-1]+1):\n","                            span_tokens[k,i+1] = 1\n","                    else:\n","                        for i in range(toks[0], toks[-1]+1):\n","                            span_tokens[k,i+1] = 1\n","\n","        if train:\n","            if self.separated_output:\n","                return (input_ids, attention_mask, start_tokens, end_tokens, span_tokens)\n","            else:\n","                return (input_ids, attention_mask, span_tokens)\n","        else:\n","            return (input_ids, attention_mask)\n","    \n","    def build_model(self):\n","        '''\n","        Add layer on top of BERT\n","        '''        \n","        ids = Input((self.max_length,), dtype=tf.int32)\n","        att = Input((self.max_length,), dtype=tf.int32)\n","        #tok = Input((self.length,), dtype=tf.int32)\n","        \n","        config = RobertaConfig.from_pretrained(self.config_file)\n","        config.output_hidden_states=True\n","        bert_model = TFRobertaModel.from_pretrained(self.bert_model_file,config=config)\n","        \n","        x = bert_model(ids,attention_mask=att)\n","        \n","        def output_layer(bert_output, name='start', activation='softmax'):\n","            x_bert = K.max(K.stack(bert_output), axis=0)\n","            x_bert = Dropout(0.5)(bert_output)\n","            # x_bert = Dropout(0.1)(bert_output)\n","            # x_bert = Conv1D(128, 2,padding='same')(x_bert)\n","            # x_bert = LeakyReLU()(x_bert)\n","            # x_bert = Conv1D(64, 2, padding='same')(x_bert)\n","            x_bert = Dense(1)(x_bert)                      \n","            x_output = Flatten()(x_bert)\n","            x_output = Activation(activation, name=name)(x_output)\n","             \n","            return x_output\n","        \n","        if self.separated_output:\n","            x1_output = output_layer(x[0], name='start')\n","            x2_output = output_layer(x[0], name='end')\n","            #x1_output = output_layer([x[-1], x[-2], x[-3]], name='start')\n","            #x2_output = output_layer([x[-1], x[-2], x[-3]], name='end')\n","            x_output = output_layer(x[0], name='span', activation='sigmoid')   \n","    \n","            model = Model(inputs=[ids, att], outputs=[x1_output,x2_output, x_output])\n","            # optimizer = Adam(learning_rate=3e-5)\n","            # model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n","        else:\n","            x_output = output_layer(x[0], name='span', activation='sigmoid')   \n","            model = Model(inputs=[ids, att], outputs=x_output)\n","\n","        optimizer = Adam(learning_rate=2.e-5)\n","        #optimizer = Adam(learning_rate=3.0e-5)\n","        model.compile(loss=self.loss_fn, loss_weights = self.loss_weights, optimizer=optimizer)\n","        #model.compile(loss=jaccard_distance, optimizer=optimizer)\n","        \n","        # print(model.summary)\n","        # K.clear_session()\n","            \n","        return model\n","    \n","    def predict_decode(self, text_data, preds, vec_idx):\n","        all_selected_text = []\n","        ii = 0\n","        if self.separated_output:\n","            start = preds[0]\n","            end = preds[1]\n","        else:\n","            span = preds[0]\n","\n","        for k in vec_idx:\n","            if self.separated_output:\n","                a = np.argmax(start[k,])\n","                b = np.argmax(end[k,])\n","            else:\n","                min_prob = np.min(span[k, ])\n","                max_prob = np.max(span[k, ])\n","                span_idx = np.where(span[k, ]>0.5*(max_prob-min_prob)+min_prob)[0]\n","                if len(span_idx)==0:\n","                    a=0\n","                    b=-1\n","                else:\n","                    a = span_idx[0]\n","                    b = span_idx[-1]\n","            if a>b: \n","                ii = ii + 1\n","                st = text_data.loc[k,'text']\n","            else:\n","                text1 = \" \"+\" \".join(text_data.loc[k,'text'].split())\n","                enc = self.tokenizer.encode(text1)\n","                st = self.tokenizer.decode(enc.ids[a-1:b])\n","            all_selected_text.append(st)\n","        print(ii)\n","        return all_selected_text\n","\n","    def train_model(self, train, epochs=3, batch_size=32, version='v0', model_path=None, verbose=1):\n","        # USE verbose=1 FOR INTERACTIVE\n","        train = train.reset_index(drop=True)\n","        if self.separated_output:\n","            input_ids, attention_mask, start_tokens, end_tokens, span_tokens = self.bert_data_transform(train, train=True)\n","            oof_start = np.zeros((input_ids.shape[0],self.max_length))\n","            oof_end = np.zeros((input_ids.shape[0],self.max_length))\n","        else:\n","            input_ids, attention_mask, span_tokens = self.bert_data_transform(train, train=True)\n","            oof_span = np.zeros((input_ids.shape[0],self.max_length))\n","\n","        jacs = [] \n","                \n","        if self.n_splits == 1:\n","            rs = ShuffleSplit(n_splits=1, test_size=0.25, random_state=self.seed)\n","        else:\n","            rs = StratifiedKFold(n_splits=self.n_splits,shuffle=True,random_state=self.seed)\n","            \n","        for fold,(idxT,idxV) in enumerate(rs.split(input_ids,train.sentiment.values)):\n","            print('#'*25)\n","            print('### FOLD %i'%(fold+1))\n","            print('#'*25)\n","           \n","            K.clear_session()\n","            model = self.build_model()\n","            print(model.summary)\n","\n","            if model_path is None:\n","                print('Training model...')\n","                #reduce_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)\n","                #et = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto')\n","                sv = ModelCheckpoint('%s-roberta-%i.h5'%(version,fold), monitor='val_loss', verbose=1, save_best_only=True,\n","                                     save_weights_only=True, mode='auto', save_freq='epoch')\n","                if self.separated_output:\n","                    targetT = {'start':start_tokens[idxT,], 'end':end_tokens[idxT,], 'span':span_tokens[idxT,]}\n","                    targetV = {'start':start_tokens[idxV,], 'end':end_tokens[idxV,], 'span':span_tokens[idxV,]}\n","                else:\n","                    targetT = span_tokens[idxT,]\n","                    targetV = span_tokens[idxV,]\n","\n","                model.fit([input_ids[idxT,], attention_mask[idxT,]], targetT, \n","                          epochs=epochs, batch_size=batch_size, verbose=verbose, callbacks=[sv],\n","                          validation_data=([input_ids[idxV,],attention_mask[idxV,]], targetV))\n","                #print('Loading model...')\n","                #model.load_weights('%s-roberta-%i.h5'%(version,fold))\n","            else:\n","                print('Loading model...')\n","                model.load_weights('%s/%s-roberta-%i.h5'%(model_path,version,fold))\n","            \n","            for label, idx in {'INF':idxT, 'OOF':idxV}.items():\n","                print('Predicting %s...'%label)\n","                if self.separated_output:\n","                    oof_start[idx,], oof_end[idx,], _ = model.predict([input_ids[idx,],attention_mask[idx,]],verbose=verbose)\n","                else:\n","                    oof_span[idx,] = model.predict([input_ids[idx,],attention_mask[idx,]],verbose=verbose)\n","                # DISPLAY FOLD JACCARD\n","                if self.separated_output:\n","                    all_selected_text = self.predict_decode(train, [oof_start, oof_end], idx)\n","                else:\n","                    all_selected_text = self.predict_decode(train, [oof_span], idx)\n","                all_jac = []\n","                for i in range(len(idx)):\n","                    all_jac.append(jaccard(all_selected_text[i],train.loc[idx[i],'selected_text']))\n","                print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all_jac))\n","                print()\n","                \n","                if label == 'OOF':\n","                    jacs.append(np.mean(all_jac))\n","                                 \n","        print('>>>> OVERALL %i Fold CV Jaccard ='%self.n_splits,np.mean(jacs))\n","        \n","        \n","    def predict_test(self, test, version='v0', model_path=None, verbose=1):\n","        test = test.reset_index(drop=True)\n","        input_ids, attention_mask = self.bert_data_transform(test, train=False)\n","        \n","        if self.separated_output:\n","            preds_start = np.zeros((input_ids.shape[0],self.max_length))\n","            preds_end = np.zeros((input_ids.shape[0],self.max_length))\n","        else:\n","            preds_span = np.zeros((input_ids.shape[0],self.max_length))\n","\n","        for fold in range(self.n_splits):\n","            K.clear_session()\n","            model = self.build_model()\n","            if model_path is None:\n","                model.load_weights('%s-roberta-%i.h5'%(version,fold))\n","            else:\n","                model.load_weights('%s/%s-roberta-%i.h5'%(model_path,version,fold))\n","            print('Predicting Test...')\n","            preds = model.predict([input_ids,attention_mask],verbose=verbose)\n","            if self.separated_output:\n","                preds_start += preds[0]/self.n_splits\n","                preds_end += preds[1]/self.n_splits\n","            else:\n","                preds_span += preds/self.n_splits\n","        \n","        if self.separated_output:                \n","            all_selected_text = self.predict_decode(test, [preds_start, preds_end], test.index)\n","        else:\n","            all_selected_text = self.predict_decode(test, [preds_span], test.index)\n","\n","        return all_selected_text"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zCNS-mliAGrI","colab_type":"text"},"source":["# Load data"]},{"cell_type":"code","metadata":{"trusted":true,"id":"1Kx79ayDAGrI","colab_type":"code","colab":{}},"source":["train = pd.read_csv(ROOT_PATH + '/input/tweet-sentiment-extraction/train.csv').fillna('')\n","test = pd.read_csv(ROOT_PATH + '/input/tweet-sentiment-extraction/test.csv').fillna('')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"qBMTQYOqAGrN","colab_type":"code","outputId":"ca582d23-40dc-4208-80ed-4c25e7d1f615","executionInfo":{"status":"ok","timestamp":1590773715631,"user_tz":240,"elapsed":1292,"user":{"displayName":"Dechang Chen","photoUrl":"","userId":"17152161433207141669"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["train.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(27481, 4)"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"trusted":true,"id":"k6tDiMlLAGrQ","colab_type":"code","outputId":"5a1f6f29-c07d-4764-c85d-7b8b38142f09","executionInfo":{"status":"ok","timestamp":1590773715632,"user_tz":240,"elapsed":1282,"user":{"displayName":"Dechang Chen","photoUrl":"","userId":"17152161433207141669"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["train.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>textID</th>\n","      <th>text</th>\n","      <th>selected_text</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>cb774db0d1</td>\n","      <td>I`d have responded, if I were going</td>\n","      <td>I`d have responded, if I were going</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>549e992a42</td>\n","      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n","      <td>Sooo SAD</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>088c60f138</td>\n","      <td>my boss is bullying me...</td>\n","      <td>bullying me</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>9642c003ef</td>\n","      <td>what interview! leave me alone</td>\n","      <td>leave me alone</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>358bd9e861</td>\n","      <td>Sons of ****, why couldn`t they put them on t...</td>\n","      <td>Sons of ****,</td>\n","      <td>negative</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       textID  ... sentiment\n","0  cb774db0d1  ...   neutral\n","1  549e992a42  ...  negative\n","2  088c60f138  ...  negative\n","3  9642c003ef  ...  negative\n","4  358bd9e861  ...  negative\n","\n","[5 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"S39fn5AjfwXr","colab_type":"code","outputId":"17a2a6dc-42ff-4073-b6ae-be696930708d","executionInfo":{"status":"ok","timestamp":1590773716297,"user_tz":240,"elapsed":1935,"user":{"displayName":"Dechang Chen","photoUrl":"","userId":"17152161433207141669"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["dummy_jacs = []\n","for i in range(train.shape[0]):\n","  jac = jaccard(train.loc[i, 'text'], train.loc[i, 'selected_text'])\n","  dummy_jacs.append(jac)\n","np.mean(dummy_jacs)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.589073146730252"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"trusted":true,"id":"muS-eLAQAGrU","colab_type":"code","outputId":"ea762a2f-fd3a-4345-aeb6-d45190beb984","executionInfo":{"status":"ok","timestamp":1590773716298,"user_tz":240,"elapsed":1923,"user":{"displayName":"Dechang Chen","photoUrl":"","userId":"17152161433207141669"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["test.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>textID</th>\n","      <th>text</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>f87dea47db</td>\n","      <td>Last session of the day  http://twitpic.com/67ezh</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>96d74cb729</td>\n","      <td>Shanghai is also really exciting (precisely -...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>eee518ae67</td>\n","      <td>Recession hit Veronique Branquinho, she has to...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>01082688c6</td>\n","      <td>happy bday!</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>33987a8ee5</td>\n","      <td>http://twitpic.com/4w75p - I like it!!</td>\n","      <td>positive</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       textID                                               text sentiment\n","0  f87dea47db  Last session of the day  http://twitpic.com/67ezh   neutral\n","1  96d74cb729   Shanghai is also really exciting (precisely -...  positive\n","2  eee518ae67  Recession hit Veronique Branquinho, she has to...  negative\n","3  01082688c6                                        happy bday!  positive\n","4  33987a8ee5             http://twitpic.com/4w75p - I like it!!  positive"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"T-6SzG16AGrZ","colab_type":"text"},"source":["# Setup"]},{"cell_type":"code","metadata":{"trusted":true,"id":"60e-fThAAGra","colab_type":"code","colab":{}},"source":["PATH = ROOT_PATH + '/input/tfrobert/'\n","tokenizer = tokenizers.ByteLevelBPETokenizer(\n","    vocab_file=PATH+'/roberta-base-vocab.json', \n","    merges_file=PATH+'/roberta-base-merges.txt', \n","    lowercase=True,\n","    add_prefix_space=True\n",")\n","\n","# config = RobertaConfig.from_pretrained(PATH+'roberta-base-config.json')\n","# bert_model = TFRobertaModel.from_pretrained(PATH+'roberta-base-tf_model.h5',config=config)\n","\n","# config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n","# bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n","\n","SAMPLE_RUN = False # Set True if you just want to debug implementation. Otherwise False.\n","MODEL_PATH = None #ROOT_PATH+'/input/models/' Set None if you want to train a new model, otherwise specify the PATH of trained model\n","N_SPLITS = 5 # 1 means train validation split at portion 0.75:0.25\n","MAX_LEN = 128\n","SEP_OUTPUT = True # True: if separate predictions for start and end, Flase: predict everything at all"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tixkicfZAGrd","colab_type":"text"},"source":["# Train model"]},{"cell_type":"code","metadata":{"id":"FwmxCzyf7Pwc","colab_type":"code","colab":{}},"source":["def jaccard_distance(y_true, y_pred, smooth=1):\n","    y_true = tf.keras.backend.cast(y_true, dtype='float32')\n","    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n","    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n","    #jac = (intersection + smooth) / (sum_ - intersection + smooth)\n","    return tf.reduce_mean(-K.log(intersection + smooth) + K.log(sum_ - intersection + smooth))\n","\n","def smoothed_crossentropy(y_true, y_pred):\n","    # adjust the targets for sequence bucketing\n","    ll = tf.shape(y_pred)[1]\n","    y_true = y_true[:, :ll]\n","    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred,\n","        from_logits=False, label_smoothing=0.1)\n","    loss = tf.reduce_mean(loss)\n","    return loss\n","\n","losses = {'start':'categorical_crossentropy', 'end':'categorical_crossentropy', 'span':'binary_crossentropy'}\n","loss_weights = {'start':1.0, 'end':1.0, 'span':2.0}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"Trxw6yUBAGre","colab_type":"code","outputId":"1b04a4df-f14b-4111-d06f-19e88c4ac5f0","executionInfo":{"status":"ok","timestamp":1590824621057,"user_tz":240,"elapsed":8676987,"user":{"displayName":"Dechang Chen","photoUrl":"","userId":"17152161433207141669"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["SEED = 43\n","tf.random.set_seed(SEED)\n","np.random.seed(SEED)\n","\n","if SAMPLE_RUN:\n","    sample_train = train.sample(2000, random_state=SEED)\n","    sample_test = test.sample(3, random_state=SEED)\n","\n","    sample_model = FineTuneBertForQA(tokenizer, config_file=PATH+'roberta-base-config.json', \n","                                       bert_model_file=PATH+'roberta-base-tf_model.h5', \n","                                       max_length=MAX_LEN, n_splits=N_SPLITS, loss_fn=losses, separated_output=SEP_OUTPUT, seed=SEED)\n","    sample_model.train_model(sample_train, epochs=2, batch_size=32, version='vs', model_path=MODEL_PATH, verbose=1)\n","    \n","    sample_selected_text = sample_model.predict_test(sample_test, version='vs', model_path=MODEL_PATH, verbose=1)\n","    sample_test['selected_text'] = sample_selected_text\n","    pd.set_option('max_colwidth', 60)\n","    print(sample_test)\n","else:\n","    full_model = FineTuneBertForQA(tokenizer, config_file=PATH+'config-roberta-base.json', \n","                                     bert_model_file=PATH+'pretrained-roberta-base.h5', \n","                                     max_length=MAX_LEN, n_splits=N_SPLITS, loss_fn=losses, loss_weights = loss_weights, separated_output=SEP_OUTPUT, seed=SEED)\n","    full_model.train_model(train, epochs=4, batch_size=16, version='v5', model_path=MODEL_PATH, verbose=1)\n","\n","    all_selected_text = full_model.predict_test(test, version='v5', model_path=MODEL_PATH, verbose=1)\n","    test['selected_text'] = all_selected_text\n","    test[['textID','selected_text']].to_csv('submission.csv',index=False)\n","    pd.set_option('max_colwidth', 60)\n","    print(test.sample(25))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["#########################\n","### FOLD 1\n","#########################\n","<bound method Network.summary of <tensorflow.python.keras.engine.training.Model object at 0x7f0a6016ca58>>\n","Training model...\n","Epoch 1/4\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","1374/1374 [==============================] - ETA: 0s - loss: 2.4617 - start_loss: 1.1546 - end_loss: 1.1535 - span_loss: 0.0768\n","Epoch 00001: val_loss improved from inf to 1.75567, saving model to v5-roberta-0.h5\n","1374/1374 [==============================] - 372s 271ms/step - loss: 2.4617 - start_loss: 1.1546 - end_loss: 1.1535 - span_loss: 0.0768 - val_loss: 1.7557 - val_start_loss: 0.8471 - val_end_loss: 0.8100 - val_span_loss: 0.0493\n","Epoch 2/4\n","1374/1374 [==============================] - ETA: 0s - loss: 1.8778 - start_loss: 0.8987 - end_loss: 0.8719 - span_loss: 0.0536\n","Epoch 00002: val_loss did not improve from 1.75567\n","1374/1374 [==============================] - 369s 268ms/step - loss: 1.8778 - start_loss: 0.8987 - end_loss: 0.8719 - span_loss: 0.0536 - val_loss: 1.7943 - val_start_loss: 0.8710 - val_end_loss: 0.8193 - val_span_loss: 0.0520\n","Epoch 3/4\n","1374/1374 [==============================] - ETA: 0s - loss: 1.6844 - start_loss: 0.8207 - end_loss: 0.7645 - span_loss: 0.0496\n","Epoch 00003: val_loss did not improve from 1.75567\n","1374/1374 [==============================] - 369s 269ms/step - loss: 1.6844 - start_loss: 0.8207 - end_loss: 0.7645 - span_loss: 0.0496 - val_loss: 1.7997 - val_start_loss: 0.8559 - val_end_loss: 0.8528 - val_span_loss: 0.0455\n","Epoch 4/4\n","1374/1374 [==============================] - ETA: 0s - loss: 1.7775 - start_loss: 0.8200 - end_loss: 0.8577 - span_loss: 0.0499\n","Epoch 00004: val_loss did not improve from 1.75567\n","1374/1374 [==============================] - 369s 269ms/step - loss: 1.7775 - start_loss: 0.8200 - end_loss: 0.8577 - span_loss: 0.0499 - val_loss: 1.9936 - val_start_loss: 0.8842 - val_end_loss: 1.0132 - val_span_loss: 0.0481\n","Predicting INF...\n","687/687 [==============================] - 101s 148ms/step\n","325\n",">>>> FOLD 1 Jaccard = 0.7038364941112997\n","\n","Predicting OOF...\n","172/172 [==============================] - 28s 163ms/step\n","93\n",">>>> FOLD 1 Jaccard = 0.6658118442403039\n","\n","#########################\n","### FOLD 2\n","#########################\n","<bound method Network.summary of <tensorflow.python.keras.engine.training.Model object at 0x7f08f9f65390>>\n","Training model...\n","Epoch 1/4\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","1375/1375 [==============================] - ETA: 0s - loss: 2.4336 - start_loss: 1.1469 - end_loss: 1.1412 - span_loss: 0.0727\n","Epoch 00001: val_loss improved from inf to 1.82744, saving model to v5-roberta-1.h5\n","1375/1375 [==============================] - 392s 285ms/step - loss: 2.4336 - start_loss: 1.1469 - end_loss: 1.1412 - span_loss: 0.0727 - val_loss: 1.8274 - val_start_loss: 0.9134 - val_end_loss: 0.8157 - val_span_loss: 0.0492\n","Epoch 2/4\n","1375/1375 [==============================] - ETA: 0s - loss: 1.8061 - start_loss: 0.8823 - end_loss: 0.8215 - span_loss: 0.0511\n","Epoch 00002: val_loss improved from 1.82744 to 1.71165, saving model to v5-roberta-1.h5\n","1375/1375 [==============================] - 390s 283ms/step - loss: 1.8061 - start_loss: 0.8823 - end_loss: 0.8215 - span_loss: 0.0511 - val_loss: 1.7117 - val_start_loss: 0.8579 - val_end_loss: 0.7631 - val_span_loss: 0.0453\n","Epoch 3/4\n","1375/1375 [==============================] - ETA: 0s - loss: 1.6331 - start_loss: 0.7977 - end_loss: 0.7397 - span_loss: 0.0478\n","Epoch 00003: val_loss did not improve from 1.71165\n","1375/1375 [==============================] - 388s 282ms/step - loss: 1.6331 - start_loss: 0.7977 - end_loss: 0.7397 - span_loss: 0.0478 - val_loss: 1.7157 - val_start_loss: 0.8515 - val_end_loss: 0.7749 - val_span_loss: 0.0447\n","Epoch 4/4\n","1375/1375 [==============================] - ETA: 0s - loss: 1.4929 - start_loss: 0.7334 - end_loss: 0.6681 - span_loss: 0.0457\n","Epoch 00004: val_loss did not improve from 1.71165\n","1375/1375 [==============================] - 388s 282ms/step - loss: 1.4929 - start_loss: 0.7334 - end_loss: 0.6681 - span_loss: 0.0457 - val_loss: 1.7901 - val_start_loss: 0.8855 - val_end_loss: 0.8149 - val_span_loss: 0.0449\n","Predicting INF...\n","688/688 [==============================] - 113s 164ms/step\n","18\n",">>>> FOLD 2 Jaccard = 0.7668811796511458\n","\n","Predicting OOF...\n","172/172 [==============================] - 28s 164ms/step\n","6\n",">>>> FOLD 2 Jaccard = 0.7024038853398407\n","\n","#########################\n","### FOLD 3\n","#########################\n","<bound method Network.summary of <tensorflow.python.keras.engine.training.Model object at 0x7f07f4f70828>>\n","Training model...\n","Epoch 1/4\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","1375/1375 [==============================] - ETA: 0s - loss: 2.5236 - start_loss: 1.1531 - end_loss: 1.2200 - span_loss: 0.0752\n","Epoch 00001: val_loss improved from inf to 1.78309, saving model to v5-roberta-2.h5\n","1375/1375 [==============================] - 392s 285ms/step - loss: 2.5236 - start_loss: 1.1531 - end_loss: 1.2200 - span_loss: 0.0752 - val_loss: 1.7831 - val_start_loss: 0.8451 - val_end_loss: 0.8420 - val_span_loss: 0.0480\n","Epoch 2/4\n","1375/1375 [==============================] - ETA: 0s - loss: 1.8269 - start_loss: 0.8826 - end_loss: 0.8410 - span_loss: 0.0517\n","Epoch 00002: val_loss improved from 1.78309 to 1.67250, saving model to v5-roberta-2.h5\n","1375/1375 [==============================] - 389s 283ms/step - loss: 1.8269 - start_loss: 0.8826 - end_loss: 0.8410 - span_loss: 0.0517 - val_loss: 1.6725 - val_start_loss: 0.8213 - val_end_loss: 0.7598 - val_span_loss: 0.0457\n","Epoch 3/4\n","1375/1375 [==============================] - ETA: 0s - loss: 1.6550 - start_loss: 0.8066 - end_loss: 0.7528 - span_loss: 0.0478\n","Epoch 00003: val_loss did not improve from 1.67250\n","1375/1375 [==============================] - 388s 282ms/step - loss: 1.6550 - start_loss: 0.8066 - end_loss: 0.7528 - span_loss: 0.0478 - val_loss: 1.6912 - val_start_loss: 0.8289 - val_end_loss: 0.7724 - val_span_loss: 0.0450\n","Epoch 4/4\n","1375/1375 [==============================] - ETA: 0s - loss: 1.5033 - start_loss: 0.7321 - end_loss: 0.6801 - span_loss: 0.0455\n","Epoch 00004: val_loss did not improve from 1.67250\n","1375/1375 [==============================] - 388s 282ms/step - loss: 1.5033 - start_loss: 0.7321 - end_loss: 0.6801 - span_loss: 0.0455 - val_loss: 1.7332 - val_start_loss: 0.8486 - val_end_loss: 0.7932 - val_span_loss: 0.0457\n","Predicting INF...\n","688/688 [==============================] - 112s 163ms/step\n","11\n",">>>> FOLD 3 Jaccard = 0.7756392526103352\n","\n","Predicting OOF...\n","172/172 [==============================] - 28s 163ms/step\n","4\n",">>>> FOLD 3 Jaccard = 0.7042859457102942\n","\n","#########################\n","### FOLD 4\n","#########################\n","<bound method Network.summary of <tensorflow.python.keras.engine.training.Model object at 0x7f09ba569208>>\n","Training model...\n","Epoch 1/4\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","1375/1375 [==============================] - ETA: 0s - loss: 2.4007 - start_loss: 1.1298 - end_loss: 1.1198 - span_loss: 0.0755\n","Epoch 00001: val_loss improved from inf to 1.77878, saving model to v5-roberta-3.h5\n","1375/1375 [==============================] - 392s 285ms/step - loss: 2.4007 - start_loss: 1.1298 - end_loss: 1.1198 - span_loss: 0.0755 - val_loss: 1.7788 - val_start_loss: 0.8469 - val_end_loss: 0.8362 - val_span_loss: 0.0479\n","Epoch 2/4\n","1375/1375 [==============================] - ETA: 0s - loss: 1.7845 - start_loss: 0.8665 - end_loss: 0.8179 - span_loss: 0.0500\n","Epoch 00002: val_loss did not improve from 1.77878\n","1375/1375 [==============================] - 388s 282ms/step - loss: 1.7845 - start_loss: 0.8665 - end_loss: 0.8179 - span_loss: 0.0500 - val_loss: 1.8179 - val_start_loss: 0.8182 - val_end_loss: 0.9059 - val_span_loss: 0.0469\n","Epoch 3/4\n","1375/1375 [==============================] - ETA: 0s - loss: 1.6956 - start_loss: 0.7927 - end_loss: 0.8078 - span_loss: 0.0475\n","Epoch 00003: val_loss did not improve from 1.77878\n","1375/1375 [==============================] - 387s 282ms/step - loss: 1.6956 - start_loss: 0.7927 - end_loss: 0.8078 - span_loss: 0.0475 - val_loss: 1.7788 - val_start_loss: 0.8100 - val_end_loss: 0.8758 - val_span_loss: 0.0465\n","Epoch 4/4\n","1375/1375 [==============================] - ETA: 0s - loss: 1.4989 - start_loss: 0.7329 - end_loss: 0.6762 - span_loss: 0.0449\n","Epoch 00004: val_loss did not improve from 1.77878\n","1375/1375 [==============================] - 388s 282ms/step - loss: 1.4989 - start_loss: 0.7329 - end_loss: 0.6762 - span_loss: 0.0449 - val_loss: 1.7847 - val_start_loss: 0.8299 - val_end_loss: 0.8633 - val_span_loss: 0.0458\n","Predicting INF...\n","688/688 [==============================] - 113s 164ms/step\n","42\n",">>>> FOLD 4 Jaccard = 0.7772464396458771\n","\n","Predicting OOF...\n","172/172 [==============================] - 28s 163ms/step\n","9\n",">>>> FOLD 4 Jaccard = 0.7090780646258285\n","\n","#########################\n","### FOLD 5\n","#########################\n","<bound method Network.summary of <tensorflow.python.keras.engine.training.Model object at 0x7f07f5bcbc18>>\n","Training model...\n","Epoch 1/4\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n","1375/1375 [==============================] - ETA: 0s - loss: 2.7309 - start_loss: 1.2056 - end_loss: 1.3664 - span_loss: 0.0794\n","Epoch 00001: val_loss improved from inf to 2.26097, saving model to v5-roberta-4.h5\n","1375/1375 [==============================] - 392s 285ms/step - loss: 2.7309 - start_loss: 1.2056 - end_loss: 1.3664 - span_loss: 0.0794 - val_loss: 2.2610 - val_start_loss: 0.9855 - val_end_loss: 1.1718 - val_span_loss: 0.0518\n","Epoch 2/4\n","1375/1375 [==============================] - ETA: 0s - loss: 2.2194 - start_loss: 0.9740 - end_loss: 1.1363 - span_loss: 0.0545\n","Epoch 00002: val_loss improved from 2.26097 to 1.98612, saving model to v5-roberta-4.h5\n","1375/1375 [==============================] - 390s 284ms/step - loss: 2.2194 - start_loss: 0.9740 - end_loss: 1.1363 - span_loss: 0.0545 - val_loss: 1.9861 - val_start_loss: 0.8912 - val_end_loss: 0.9934 - val_span_loss: 0.0508\n","Epoch 3/4\n","1375/1375 [==============================] - ETA: 0s - loss: 1.8282 - start_loss: 0.8608 - end_loss: 0.8655 - span_loss: 0.0510\n","Epoch 00003: val_loss improved from 1.98612 to 1.84978, saving model to v5-roberta-4.h5\n","1375/1375 [==============================] - 389s 283ms/step - loss: 1.8282 - start_loss: 0.8608 - end_loss: 0.8655 - span_loss: 0.0510 - val_loss: 1.8498 - val_start_loss: 0.8952 - val_end_loss: 0.8587 - val_span_loss: 0.0479\n","Epoch 4/4\n","1375/1375 [==============================] - ETA: 0s - loss: 1.6774 - start_loss: 0.8063 - end_loss: 0.7755 - span_loss: 0.0479\n","Epoch 00004: val_loss improved from 1.84978 to 1.80994, saving model to v5-roberta-4.h5\n","1375/1375 [==============================] - 390s 283ms/step - loss: 1.6774 - start_loss: 0.8063 - end_loss: 0.7755 - span_loss: 0.0479 - val_loss: 1.8099 - val_start_loss: 0.8820 - val_end_loss: 0.8343 - val_span_loss: 0.0468\n","Predicting INF...\n","688/688 [==============================] - 113s 164ms/step\n","33\n",">>>> FOLD 5 Jaccard = 0.7375270656454447\n","\n","Predicting OOF...\n","172/172 [==============================] - 28s 164ms/step\n","19\n",">>>> FOLD 5 Jaccard = 0.6991186933780974\n","\n",">>>> OVERALL 5 Fold CV Jaccard = 0.6961396866588729\n","Predicting Test...\n","111/111 [==============================] - 18s 162ms/step\n","Predicting Test...\n","111/111 [==============================] - 18s 165ms/step\n","Predicting Test...\n","111/111 [==============================] - 18s 162ms/step\n","Predicting Test...\n","111/111 [==============================] - 18s 162ms/step\n","Predicting Test...\n","111/111 [==============================] - 18s 162ms/step\n","6\n","          textID  ...                                                selected_text\n","621   983088f2b4  ...       unlike cierra, i look like poop today. whatevahh, lol.\n","891   66019ddc8f  ...                                                       thanks\n","2086  9fbf0d59cc  ...                                             i lafff the rain\n","146   f254748cdb  ...                                            happy mothers day\n","2911  3dd127f4af  ...   hey hunnie how are u?? i miss talkin to u! ty for the f...\n","1241  fcc4f3baa5  ...                                  ahh ok! enjoy! i`ll miss it\n","1533  bab26fdc24  ...                                                          wtf\n","3116  dda6145689  ...                                     cottin with emilyyyyyyyy\n","784   5389177883  ...                    _rain i wanna see her hair hows everyone?\n","2796  4d5a01ceea  ...                                         completely excellent\n","2098  dabac40552  ...                                               super excited.\n","2135  0010bcc0e2  ...                                 recovering from my operation\n","992   472c3e2c41  ...   getting somewhere with my first 'real' kiokudb and cata...\n","828   c392253b42  ...            nope no way in to stop just have to put up wiv it\n","3508  60e5c2c335  ...                                                        bored\n","884   ed298c6e5d  ...                                                       nice!!\n","125   410dd99aa3  ...   man im so sad school is ending but then again high scho...\n","3038  b4d1c8c080  ...                                                   can i help\n","323   5aa8a5280f  ...   i knooww & my hot water bottle iss in whangamata withou...\n","3210  a26a75179f  ...                                              haha agreed lol\n","964   2a35e976ea  ...                                       werd. that`s very true\n","594   b70639aec0  ...                                          poor medicated baby\n","3094  00ce730001  ...   unstable broadband and electricity taking toll on my me...\n","1624  4bc4d97503  ...                                   **** i shouldnt have left!\n","3022  3f09c16e52  ...                           it`s all good. thanks for #ff love\n","\n","[25 rows x 4 columns]\n"],"name":"stdout"}]}]}